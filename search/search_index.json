{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Deployment Automation for Atlassian DC on K8s \u00b6 Support disclaimer \u00b6 Official support warning This project is designed for Atlassian vendors to run DC App Performance Toolkit and is not officially supported. We don't recommend using the scripts to deploy production instances but they can be used for inspiration and to deploy testing stacks. Beware that the project is tested only for DCAPT framework use cases with the pre-defined configuration files only. Products and Platforms \u00b6 Available Products and Platforms The following products can be installed to Amazon Web Services : Jira Jira Service Management Confluence Bitbucket Bamboo Crowd This project can be used for bootstrapping Atlassian DC products in a Kubernetes cluster. This tool will create a Kubernetes cluster and all the required infrastructure. It will also install Atlassian DC products into this pre-provisioned cluster using the Data Center Helm Charts . Deployment overview \u00b6 The diagram below provides a high level overview of what a typical deployment will look like for each DC product: Jira Confluence Bitbucket Bamboo Crowd Architectural overview for Jira Architectural overview for Confluence Architectural overview for Bitbucket Architectural overview for Bamboo Architectural overview for Crowd Cluster size and cost Cluster Autoscaler is provisioned as part of the deployment. You can define the minimum and maximum size of the cluster. The number of nodes will be automatically adjusted depending on the workload resource requirements. Multiple deployments to a single cluster Multiple DC products can also be provisioned to the same cluster. See the Configuration guide for more details Deploying a Data Center product \u00b6 Prerequisites - steps for environment setup including installation of 3rd party tooling Configuration - steps for configuring deployment Installation - steps for running a deployment Product support \u00b6 The minimum versions that we support for each product are: Jira DC Jira Service Management DC Confluence DC Bitbucket DC Bamboo DC Crowd DC 8.19 4.20 7.13 8.17 8.1 5.1 Feedback \u00b6 If you find any issues, raise a ticket . In case of technical questions, issues or problems using this project with DC App Performance Toolkit, contact us for help in the community Slack #data-center-app-performance-toolkit channel. Contributions \u00b6 Contributions are welcome! Find out how to contribute . License \u00b6 Apache 2.0 licensed, see license file .","title":"Home"},{"location":"#deployment-automation-for-atlassian-dc-on-k8s","text":"","title":"Deployment Automation for Atlassian DC on K8s"},{"location":"#support-disclaimer","text":"Official support warning This project is designed for Atlassian vendors to run DC App Performance Toolkit and is not officially supported. We don't recommend using the scripts to deploy production instances but they can be used for inspiration and to deploy testing stacks. Beware that the project is tested only for DCAPT framework use cases with the pre-defined configuration files only.","title":"Support disclaimer"},{"location":"#products-and-platforms","text":"Available Products and Platforms The following products can be installed to Amazon Web Services : Jira Jira Service Management Confluence Bitbucket Bamboo Crowd This project can be used for bootstrapping Atlassian DC products in a Kubernetes cluster. This tool will create a Kubernetes cluster and all the required infrastructure. It will also install Atlassian DC products into this pre-provisioned cluster using the Data Center Helm Charts .","title":"Products and Platforms"},{"location":"#deployment-overview","text":"The diagram below provides a high level overview of what a typical deployment will look like for each DC product: Jira Confluence Bitbucket Bamboo Crowd Architectural overview for Jira Architectural overview for Confluence Architectural overview for Bitbucket Architectural overview for Bamboo Architectural overview for Crowd Cluster size and cost Cluster Autoscaler is provisioned as part of the deployment. You can define the minimum and maximum size of the cluster. The number of nodes will be automatically adjusted depending on the workload resource requirements. Multiple deployments to a single cluster Multiple DC products can also be provisioned to the same cluster. See the Configuration guide for more details","title":"Deployment overview"},{"location":"#deploying-a-data-center-product","text":"Prerequisites - steps for environment setup including installation of 3rd party tooling Configuration - steps for configuring deployment Installation - steps for running a deployment","title":"Deploying a Data Center product"},{"location":"#product-support","text":"The minimum versions that we support for each product are: Jira DC Jira Service Management DC Confluence DC Bitbucket DC Bamboo DC Crowd DC 8.19 4.20 7.13 8.17 8.1 5.1","title":"Product support"},{"location":"#feedback","text":"If you find any issues, raise a ticket . In case of technical questions, issues or problems using this project with DC App Performance Toolkit, contact us for help in the community Slack #data-center-app-performance-toolkit channel.","title":"Feedback"},{"location":"#contributions","text":"Contributions are welcome! Find out how to contribute .","title":"Contributions"},{"location":"#license","text":"Apache 2.0 licensed, see license file .","title":"License"},{"location":"development/HOW_TO_START/","text":"How to start development \u00b6 A number of tools have been used for building this project. When working on the Data Center Terraform project it's recommended that a dev environment is set up with the same tools. CLI Tooling A number of CLI tools are recommended for working on this project. See the Prerequisites guide for details. Golang Golang is used extensively for testing this project, as such it needs to be installed. Check if Go is already installed by running the following command: go version If Go is not installed, install it by following the official instructions . Pre-commit hook Configure pre-commit hook to maintain quality and consistency of the Terraform scripts. Install pre-commit as follows: brew install pre-commit Then configure pre-commit to use the project .pre-commit-config.yaml git add .pre-commit-config.yaml","title":"How to start development"},{"location":"development/HOW_TO_START/#how-to-start-development","text":"A number of tools have been used for building this project. When working on the Data Center Terraform project it's recommended that a dev environment is set up with the same tools. CLI Tooling A number of CLI tools are recommended for working on this project. See the Prerequisites guide for details. Golang Golang is used extensively for testing this project, as such it needs to be installed. Check if Go is already installed by running the following command: go version If Go is not installed, install it by following the official instructions . Pre-commit hook Configure pre-commit hook to maintain quality and consistency of the Terraform scripts. Install pre-commit as follows: brew install pre-commit Then configure pre-commit to use the project .pre-commit-config.yaml git add .pre-commit-config.yaml","title":"How to start development"},{"location":"development/HOW_TO_TEST/","text":"Testing \u00b6 You can find the tests in the ./unittest and ./e2etest subdirectories under /test . Unit tests \u00b6 The unittest subdirectory includes module-level terraform plan validation tests. It is required to implement the unit tests for each module. Make sure each test case covers default, customised and invalid conditions. End-to-end tests \u00b6 The e2etest subdirectory contains the end-to-end infrastructure and product tests. The tests cover the entire deployment process, including the provisioning of resources into a cloud provider. Each product will have one test function that covers all the states. The test function starts with generating configurations for a test environment. You can modify the configuration variables in the createConfig() function. The provisioning process is as follows: Create AWS resources using Terraform. Create an EKS namespace (product name by default). Clone the Atlassian Helm chart repository and install the specified product using Helm. Once the cluster and product are initialized, bambooHealthTests() function will validate the installation result. Requirements \u00b6 See the How to start development guide for details on how your environment should be setup prior to running tests. The repository also uses Terratest to run the tests. Setting up AWS security credentials \u00b6 Set up a user with an administrator IAM role. See Configuration basics \u2014 AWS Command Line Interface . Set credentials to connect to cloud provider. The project looks for ~/.aws . For more details refer to AWS cli-configure-quickstart . Running unit tests To run unit tests, use the following commands: go get -v -t -d ./... && go mod tidy go test ./test/unittest/... -v You can use regex keywords to run specific groups of test cases. For example, you can run only VPC module-related tests with go test ./unittest/... -v -run TestVpc . Running end-to-end tests End-to-end tests take approx. 40\u201360 min. to complete. To run end-to-end tests, use the following commands: export TF_VAR_bamboo_license = '<bamboo-license>' mkdir -p ./test/e2etest/artifacts go get -v -t -d ./... && go mod tidy go test ./test/e2etest -v -timeout 60m -run Installer | tee ./test/e2etest/artifacts/e2etest.log GitHub Actions \u00b6 These unit and end-to-end tests run as part of the GitHub Actions setup for this repo . You can find the configuration file for these actions in .github/workflows within the root level of the project.","title":"Testing"},{"location":"development/HOW_TO_TEST/#testing","text":"You can find the tests in the ./unittest and ./e2etest subdirectories under /test .","title":"Testing"},{"location":"development/HOW_TO_TEST/#unit-tests","text":"The unittest subdirectory includes module-level terraform plan validation tests. It is required to implement the unit tests for each module. Make sure each test case covers default, customised and invalid conditions.","title":"Unit tests"},{"location":"development/HOW_TO_TEST/#end-to-end-tests","text":"The e2etest subdirectory contains the end-to-end infrastructure and product tests. The tests cover the entire deployment process, including the provisioning of resources into a cloud provider. Each product will have one test function that covers all the states. The test function starts with generating configurations for a test environment. You can modify the configuration variables in the createConfig() function. The provisioning process is as follows: Create AWS resources using Terraform. Create an EKS namespace (product name by default). Clone the Atlassian Helm chart repository and install the specified product using Helm. Once the cluster and product are initialized, bambooHealthTests() function will validate the installation result.","title":"End-to-end tests"},{"location":"development/HOW_TO_TEST/#requirements","text":"See the How to start development guide for details on how your environment should be setup prior to running tests. The repository also uses Terratest to run the tests.","title":"Requirements"},{"location":"development/HOW_TO_TEST/#setting-up-aws-security-credentials","text":"Set up a user with an administrator IAM role. See Configuration basics \u2014 AWS Command Line Interface . Set credentials to connect to cloud provider. The project looks for ~/.aws . For more details refer to AWS cli-configure-quickstart . Running unit tests To run unit tests, use the following commands: go get -v -t -d ./... && go mod tidy go test ./test/unittest/... -v You can use regex keywords to run specific groups of test cases. For example, you can run only VPC module-related tests with go test ./unittest/... -v -run TestVpc . Running end-to-end tests End-to-end tests take approx. 40\u201360 min. to complete. To run end-to-end tests, use the following commands: export TF_VAR_bamboo_license = '<bamboo-license>' mkdir -p ./test/e2etest/artifacts go get -v -t -d ./... && go mod tidy go test ./test/e2etest -v -timeout 60m -run Installer | tee ./test/e2etest/artifacts/e2etest.log","title":"Setting up AWS security credentials"},{"location":"development/HOW_TO_TEST/#github-actions","text":"These unit and end-to-end tests run as part of the GitHub Actions setup for this repo . You can find the configuration file for these actions in .github/workflows within the root level of the project.","title":"GitHub Actions"},{"location":"troubleshooting/LIMITATIONS/","text":"Limitations \u00b6 Products and Platforms \u00b6 Available Products and Platforms Current project limitations listed below: AWS is the only supported cloud provider. Jira , Jira Service Management , Confluence , Bitbucket , Bamboo , Crowd are the DC products supported by this project. Bitbucket scaling up issue with NFS \u00b6 There is an intermittent issue when scaling the Bitbucket cluster up results in new pods not being able to acquire a lock on the shared home. Log file: bitbucket have write permission on that directory? Is file locking enabled for the filesystem? java.io.IOException: No locks available at java.base/sun.nio.ch.FileDispatcherImpl.lock0(Native Method) at java.base/sun.nio.ch.FileDispatcherImpl.lock(FileDispatcherImpl.java:96) at java.base/sun.nio.ch.FileChannelImpl.tryLock(FileChannelImpl.java:1161) at com.atlassian.stash.internal.home.HomeLock.acquireLock(HomeLock.java:112) at com.atlassian.stash.internal.home.HomeLock.lock(HomeLock.java:98) at com.atlassian.stash.internal.home.HomeLockAcquirer.lock(HomeLockAcquirer.java:58) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:918) at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:583) at javax.servlet.GenericServlet.init(GenericServlet.java:158) at java.base/java.lang.Thread.run(Thread.java:829) ... 37 frames trimmed 2022-03-22 05:38:46,125 WARN [spring-startup] o.s.w.c.s.XmlWebApplicationContext Exception encountered during context initialization - cancelling refresh attempt: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'crowdAliasDao': Unsatisfied dependency expressed through method 'setSessionFactory' parameter 0; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sharedHomeLockAcquirer' defined in class path resource [stash-context.xml]: Invocation of init method failed; nested exception is com.atlassian.stash.internal.home.HomeLockFailedException: Unable to create and acquire shared lock file '/var/atlassian/application-data/shared-home/.lock' for Bitbucket shared home directory '/var/atlassian/application-data/shared-home'. Please ensure that the user running Bitbucket has permission to write to this directory and that file locking is enabled for your network file system. If this is already the case, please check the logs for more information. 2022-03-22 05:38:46,136 INFO [spring-startup] c.a.s.internal.home.HomeLockAcquirer Releasing lock on /var/atlassian/application-data/bitbucket This issue is intermittent and is occasionally exhibited when scaling the pod count from 1 to 2 . If you pre-seed the Bitbucket instance and set bitbucket_replica_count to 2 from the beginning, no issue will occur. When encountered, the issue can be resolved by killing the Bitbucket and NFS pods and waiting for them to become available again. kubectl delete pod bitbucket-0 -n atlassian kubectl delete pod bitbucket-nfs-server-0 -n atlassian Infrastructure limitations \u00b6 Cloud provider \u00b6 Amazon Web Services (AWS) is the only supported cloud platform. Database \u00b6 PostgreSQL is the defined database engine for the products and cannot be modified in the configuration. However, users can change the database instance type and storage size . Domain \u00b6 Stick to either domain or no domain for the whole deployment, i.e. configure domain = \"<example.com>\" value or leave it commented. Switching between domain and no domain is not supported. Deployment limitations \u00b6 Destroying product by removing product from products list and then running another install is not supported as it will result in unexpected behaviour on the following re-installation.","title":"Limitations"},{"location":"troubleshooting/LIMITATIONS/#limitations","text":"","title":"Limitations"},{"location":"troubleshooting/LIMITATIONS/#products-and-platforms","text":"Available Products and Platforms Current project limitations listed below: AWS is the only supported cloud provider. Jira , Jira Service Management , Confluence , Bitbucket , Bamboo , Crowd are the DC products supported by this project.","title":"Products and Platforms"},{"location":"troubleshooting/LIMITATIONS/#bitbucket-scaling-up-issue-with-nfs","text":"There is an intermittent issue when scaling the Bitbucket cluster up results in new pods not being able to acquire a lock on the shared home. Log file: bitbucket have write permission on that directory? Is file locking enabled for the filesystem? java.io.IOException: No locks available at java.base/sun.nio.ch.FileDispatcherImpl.lock0(Native Method) at java.base/sun.nio.ch.FileDispatcherImpl.lock(FileDispatcherImpl.java:96) at java.base/sun.nio.ch.FileChannelImpl.tryLock(FileChannelImpl.java:1161) at com.atlassian.stash.internal.home.HomeLock.acquireLock(HomeLock.java:112) at com.atlassian.stash.internal.home.HomeLock.lock(HomeLock.java:98) at com.atlassian.stash.internal.home.HomeLockAcquirer.lock(HomeLockAcquirer.java:58) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:918) at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:583) at javax.servlet.GenericServlet.init(GenericServlet.java:158) at java.base/java.lang.Thread.run(Thread.java:829) ... 37 frames trimmed 2022-03-22 05:38:46,125 WARN [spring-startup] o.s.w.c.s.XmlWebApplicationContext Exception encountered during context initialization - cancelling refresh attempt: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'crowdAliasDao': Unsatisfied dependency expressed through method 'setSessionFactory' parameter 0; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sharedHomeLockAcquirer' defined in class path resource [stash-context.xml]: Invocation of init method failed; nested exception is com.atlassian.stash.internal.home.HomeLockFailedException: Unable to create and acquire shared lock file '/var/atlassian/application-data/shared-home/.lock' for Bitbucket shared home directory '/var/atlassian/application-data/shared-home'. Please ensure that the user running Bitbucket has permission to write to this directory and that file locking is enabled for your network file system. If this is already the case, please check the logs for more information. 2022-03-22 05:38:46,136 INFO [spring-startup] c.a.s.internal.home.HomeLockAcquirer Releasing lock on /var/atlassian/application-data/bitbucket This issue is intermittent and is occasionally exhibited when scaling the pod count from 1 to 2 . If you pre-seed the Bitbucket instance and set bitbucket_replica_count to 2 from the beginning, no issue will occur. When encountered, the issue can be resolved by killing the Bitbucket and NFS pods and waiting for them to become available again. kubectl delete pod bitbucket-0 -n atlassian kubectl delete pod bitbucket-nfs-server-0 -n atlassian","title":"Bitbucket scaling up issue with NFS"},{"location":"troubleshooting/LIMITATIONS/#infrastructure-limitations","text":"","title":"Infrastructure limitations"},{"location":"troubleshooting/LIMITATIONS/#cloud-provider","text":"Amazon Web Services (AWS) is the only supported cloud platform.","title":"Cloud provider"},{"location":"troubleshooting/LIMITATIONS/#database","text":"PostgreSQL is the defined database engine for the products and cannot be modified in the configuration. However, users can change the database instance type and storage size .","title":"Database"},{"location":"troubleshooting/LIMITATIONS/#domain","text":"Stick to either domain or no domain for the whole deployment, i.e. configure domain = \"<example.com>\" value or leave it commented. Switching between domain and no domain is not supported.","title":"Domain"},{"location":"troubleshooting/LIMITATIONS/#deployment-limitations","text":"Destroying product by removing product from products list and then running another install is not supported as it will result in unexpected behaviour on the following re-installation.","title":"Deployment limitations"},{"location":"troubleshooting/SUPPORT_BOUNDARIES/","text":"Support boundaries \u00b6 Official support warning This project is designed for Atlassian vendors to run DCAPT performance toolkit and is not officially supported. We don't recommend using the scripts to deploy production instances but they can be used for inspiration and to deploy testing stacks. Beware that the project is tested only for DCAPT framework use case. Additional information Read our troubleshooting tips . Read about the product and platform limitations .","title":"Support boundaries"},{"location":"troubleshooting/SUPPORT_BOUNDARIES/#support-boundaries","text":"Official support warning This project is designed for Atlassian vendors to run DCAPT performance toolkit and is not officially supported. We don't recommend using the scripts to deploy production instances but they can be used for inspiration and to deploy testing stacks. Beware that the project is tested only for DCAPT framework use case. Additional information Read our troubleshooting tips . Read about the product and platform limitations .","title":"Support boundaries"},{"location":"troubleshooting/TROUBLESHOOTING/","text":"Troubleshooting tips \u00b6 This guide contains general tips on how to investigate an application deployment that doesn't work correctly. How to troubleshoot a failed Helm release installation? \u00b6 Symptom Install script fails due to failure when installing Helm release. This applies to all DC products. You will see the following error: module.confluence[0].helm_release.confluence: Still creating... [20m10s elapsed] Warning: Helm release \"confluence\" was created but has a failed status. Use the `helm` command to investigate the error, correct it, then run Terraform again. with module.confluence[0].helm_release.confluence, on modules/products/confluence/helm.tf line 4, in resource \"helm_release\" \"confluence\": 4: resource \"helm_release\" \"confluence\" { Error: timed out waiting for the condition with module.confluence[0].helm_release.confluence, on modules/products/confluence/helm.tf line 4, in resource \"helm_release\" \"confluence\": 4: resource \"helm_release\" \"confluence\" { Releasing state lock. This may take a few moments... Helm gives up waiting for a successful release, Usually, it means that Confluence (or any other product) pod failed to pass its readiness probe, or the pod is stuck in a Pending state. Solution To troubleshoot the error, run the following script: scripts/collect_k8s_logs.sh atlas-dcapt-confluence-small-cluster us-east-2 /path/to/local/directory Cluster name and region may differ (look at environment name and region in your config.tfvars ). For example, if your environment_name is dcapt-confluence-small , then your cluster name is atlas-dcapt-confluence-small-cluster . The last argument is a destination path for a tar.gz with logs that the script will produce. Share the archive in Slack #data-center-app-performance-toolkit channel along with your support request. You can also look at the pod and its logs, e.g.: confluence-0_log.log confluence-0_describe.log Odds are that logs may shed some light on why the pod isn't ready. The product_describe.log file will contain K8S events that may also help understand why the pod isn't in a Running state. It's also a good idea to get logs that are not sent to stdout/err: kubectl exec confluence-0 -n atlassian -i -t -- cat /var/atlassian/application-data/confluence/logs/atlassian-confluence.log Typically, if the pod is Running but not marked as Ready , it's the application that failed to start, i.e. it isn't an infrastructure issue. How to troubleshoot instances that failed to join the Kubernetes cluster \u00b6 Symptom When Terraform creates EKS infrastructure, EKS cluster (control plane) is created first. Once the cluster has been created, a NodeGroup (backed by an ASG) is created, and EC2 instances join the cluster as worker nodes. If a node fails to join its cluster, you will typically see the following error: Error: waiting for EKS Node Group (atlas-dcapt-jira-small-cluster:appNode-t3_xlarge-20240521085758213900000012) to create: unexpected state 'CREATE_FAILED', wanted target 'ACTIVE'. last error: 1 error occurred: * i-0dd1a9dc64303a10b: NodeCreationFailure: Instances failed to join the kubernetes cluster with module.base-infrastructure.module.eks.module.eks.module.eks_managed_node_group[\"appNodes\"].aws_eks_node_group.this[0], on .terraform/modules/base-infrastructure.eks.eks/modules/eks-managed-node-group/main.tf line 272, in resource \"aws_eks_node_group\" \"this\": 272: resource \"aws_eks_node_group\" \"this\" { There can be several reasons why nodes can't join the cluster. Permissions issues are the most common. Make sure STS is enabled for your account in the target region. With STS disabled, EKS control plane will deny join requests from the nodes. After enabling STS, destroy existing environment and re-run the installation. How to fix 'exec plugin is configured to use API version' error? \u00b6 Symptom When running install.sh script the installation fails with an error: module.base-infrastructure.kubernetes_namespace.products: Creating... module.base-infrastructure.module.ingress.helm_release.ingress: Creating... Error: Post \"https://1D2E0AC7AE5EC290740D816BD53A68AB.gr7.us-east-1.eks.amazonaws.com/api/v1/namespaces\": getting credentials: exec plugin is configured to use API version client.authentication.k8s.io/v1beta1, plugin returned version client.authentication.k8s.io/v1alpha1 with module.base-infrastructure.kubernetes_namespace.products, on modules/common/main.tf line 43, in resource \"kubernetes_namespace\" \"products\": 43: resource \"kubernetes_namespace\" \"products\" { Error: Kubernetes cluster unreachable: Get \"https://1D2E0AC7AE5EC290740D816BD53A68AB.gr7.us-east-1.eks.amazonaws.com/version\": getting credentials: decoding stdout: no kind \"ExecCredential\" is registered for version \"client.authentication.k8s.io/v1alpha1\" in scheme \"pkg/runtime/scheme.go:100\" with module.base-infrastructure.module.ingress.helm_release.ingress, on modules/AWS/ingress/main.tf line 44, in resource \"helm_release\" \"ingress\": 44: resource \"helm_release\" \"ingress\" { The error is caused by API version mismatch. AWS CLI and Kubernetes and Helm providers use v1alpha1 and v1beta1 respectively. Solution Update AWS CLI to the most recent version. How do I uninstall an environment using a different Terraform configuration file? \u00b6 Symptom If you try to uninstall an environment by using a different configuration file than the one you used to install it or by using a different version of the code, you may encounter some issues during uninstallation. In most cases, Terraform reports that the resource cannot be removed because it's in use. Solution Identify the resource and delete it manually from the AWS console, and then restart the uninstallation process. Make sure to always use the same configuration file that was used to install the environment. How do I deal with persistent volumes that do not get removed as part of the Terraform uninstallation? \u00b6 Symptom Uninstall fails to remove the persistent volume. Error: Persistent volume atlassian-dc-share-home-pv still exists ( Bound ) Error: context deadline exceeded Error: Persistent volume claim atlassian-dc-share-home-pvc still exists with Solution If a pod termination stalls, it will block pvc and pv deletion. To fix this problem we need to terminate product pod first and run uninstall command again. kubectl delete pod <stalled-pod> -n atlassian --force To see the stalled pod name you can run the following command: kubectl get pods -n atlassian How do I deal with suspended AWS Auto Scaling Groups during Terraform uninstallation? \u00b6 Symptom If for any reason Auto Scaling Group gets suspended, AWS does not allow Terraform to delete the node group. In cases like this the uninstall process gets interrupted with the following error: Error: error waiting for EKS Node Group ( atlas-<environment_name>-cluster:appNode ) to delete: unexpected state 'DELETE_FAILED' , wanted target '' . last error: 2 errors occurred: * i-06a4b4afc9e7a76b0: NodeCreationFailure: Instances failed to join the kubernetes cluster * eks-appNode-3ebedddc-2d97-ff10-6c23-4900d1d79599: AutoScalingGroupInvalidConfiguration: Couldn ' t terminate instances in ASG as Terminate process is suspended Solution Delete the reported Auto Scaling Group in AWS console and run uninstall command again. How do I deal with Terraform AWS authentication issues during installation? \u00b6 Symptom The following error is thrown: An error occurred ( ExpiredToken ) when calling the GetCallerIdentity operation: The security token included in the request is expired Solution Terraform cannot deploy resources to AWS if your security token has expired. Renew your token and retry. How do I deal with Terraform state lock acquisition errors? \u00b6 If user interrupts the installation or uninstallation process, Terraform won't be able to unlock resources. In this case, Terraform is unable to acquire state lock in the next attempt. Symptom The following error is thrown: Acquiring state lock. This may take a few moments... Error: Error acquiring the state lock Error message: ConditionalCheckFailedException: The conditional request failed Lock Info: ID: 26f7b9a8-1bef-0674-669b-1d60800dea4d Path: atlassian-data-center-terraform-state-xxxxxxxxxx/bamboo-xxxxxxxxxx/terraform.tfstate Operation: OperationTypeApply Who: xxxxxxxxxx@C02CK0JYMD6V Version: 1 .0.9 Created: 2021 -11-04 00 :50:34.736134 +0000 UTC Info: Solution Forcibly unlock the state by running the following command: terraform force-unlock <ID> Where <ID> is the value that appears in the error message. There are two Terraform locks; one for the infrastructure and another for Terraform state. If you are still experiencing lock issues, change the directory to ./modules/tfstate and retry the same command. How do I deal with state data in S3 that does not have the expected content? \u00b6 If Terraform state is locked and users forcefully unlock it using terraform force-unlock <id> , it may not get a chance to update the Digest value in DynamoDB. This prevents Terraform from reading the state data. Symptom The following error is thrown: Error refreshing state: state data in S3 does not have the expected content. This may be caused by unusually long delays in S3 processing a previous state update. Please wait for a minute or two and try again. If this problem persists, and neither S3 nor DynamoDB are experiencing an outage, you may need to manually verify the remote state and update the Digest value stored in the DynamoDB table to the following value: 531ca9bce76bbe0262f610cfc27bbf0b Solution Open DynamoDB page in AWS console and find the table named atlassian_data_center_<region>_<aws_account_id>_tf_lock in the same region as the cluster. Click on Explore Table Items and find the LockID named <table_name>/<environment_name>/terraform.tfstate-md5 . Click on the item and replace the Digest value with the given value in the error message. How do I deal with pre-existing state in multiple environment? \u00b6 If you start installing a new environment while you already have an active environment installed before, you should NOT use the pre-existing state. The same scenario when you want to uninstall a non-active environment. What is active environment? Active environment is the latest environment you installed or uninstalled. Tip Answer ' NO ' when you get a similar message during installation or uninstallation: Do you want to copy existing state to the new backend? Pre-existing state was found while migrating the previous \"s3\" backend to the newly configured \"s3\" backend. An existing non-empty state already exists in the new backend. The two states have been saved to temporary files that will be removed after responding to this query. Do you want to overwrite the state in the new backend with the previous state? Enter \"yes\" to copy and \"no\" to start with the existing state in the newly configured \"s3\" backend. Enter a value: Symptom Installation or uninstallation break after you chose to use pre-existing state. Solution Clean up the project before proceed. In root directory of the project run: ./scripts/cleanup.sh -s -t -x -r . terraform init -var-file = <config file> Then re-run the install/uninstall script. How do I deal with Module not installed error during uninstallation? \u00b6 There are some Terraform specific modules that are required when performing an uninstall. These modules are generated by Terraform during the install process and are stored in the .terraform folder. If Terraform cannot find these modules, then it won't be able perform an uninstall of the infrastructure. Symptom Error: Module not installed on main.tf line 7 : 7 : module \"tfstate-bucket\" { This module is not yet installed. Run \"terraform init\" to install all modules required by this configuration. Solution In the root directory of the project run: ./scripts/cleanup.sh -s -t -x -r . cd modules/tfstate terraform init -var-file = <config file> Go back to the root of the project and re-run the uninstall.sh script. How to deal with getting credentials: exec: executable aws failed with exit code 2 error? \u00b6 Symptom After performing an install.sh the following error is encountered: Error: Post \"https://0839E580E6ADB7B784AECE0E152D8AF2.gr7.eu-west-1.eks.amazonaws.com/api/v1/namespaces\" : getting credentials: exec: executable aws failed with exit code 2 with module.base-infrastructure.kubernetes_namespace.products, on modules/common/main.tf line 39 , in resource \"kubernetes_namespace\" \"products\" : 39 : resource \"kubernetes_namespace\" \"products\" { Solution Ensure you are using a version of the aws cli that is at least >= 2 The version can be checked by running: aws --version How to ssh to application nodes? \u00b6 Sometimes you need to ssh to the application nodes. This can be done by running: kubectl exec -it <pod-name> -n atlassian -- /bin/bash where <pod-name> is the name of the application pod you want to ssh such as bitbucket-0 or jira-1 . To get the pod names you can run: kubectl get pods -n atlassian How to access to the application log files? \u00b6 A simple way to access to the application log content is running the following command: kubectl logs <pod-name> -n atlassian where <pod-name> is the name of the application pod you want to see the log such as bitbucket-0 or jira-1 . To get the pod names you can run: kubectl get pods -n atlassian However, another approach to see the full log files produced by the application would be to ssh to the application pod and access directly the log folder. kubectl exec -it <pod-name> -n atlassian -- /bin/bash cd /var/atlassian/<application>/logs where <application> is the name of the application such as confluence , bamboo , bitbucket , or jira . Note that for some applications log foler is /var/atlassian/<application>/log and for others is /var/atlassian/<application>/logs . If you need to copy the log files to a local machine, you can use the following command: kubectl cp atlassian/<pod-name>:/var/atlassian/<application>/logs/<log_files> <local-path> How to deal with persistent volume claim destroy failed error? \u00b6 The PVC cannot be destroyed when bound to a pod. Overcome this by scaling down to 0 pods first before deleting PVC. helm upgrade PRODUCT atlassian-data-center/PRODUCT --set replicaCount=0 --reuse-values -n atlassian How to manually clean up resources when uninstall has failed? \u00b6 Sometimes Terraform is unable to destroy resources for various reasons. This normally happens at EKS level. One quick solution is to manually delete the EKS cluster, and re-run uninstall, so that Terraform will pick up from there. To delete EKS cluster, go to AWS console > EKS service > the cluster you're deploying. You'll need to go to 'Configuration' tab > 'Compute' tab > click into node group. Then in node group screen > Details > click into Autoscaling group. It'll then direct you to EC2 > Auto Scaling Group screen with the ASG selected > 'Delete' the chosen ASG. Wait for the ASG to be deleted, then go back to EKS cluster > Delete. How to deal with This object does not have an attribute named error when running uninstall.sh \u00b6 It is possible that if the installation has failed, the uninstall script will return an error like: module.base-infrastructure.module.eks.aws_autoscaling_group_tag.this[\"Name\"]: Refreshing state... [id=eks-appNode-t3_xlarge-50c26268-ea57-5aee-4523-68f33af7dd71,Name] Error: Unsupported attribute on dc-infrastructure.tf line 142, in module \"confluence\": 142: ingress = module.base-infrastructure.ingress \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2502 module.base-infrastructure is object with 5 attributes This object does not have an attribute named \"ingress\". Error: Unsupported attribute This happens because some of the modules failed to be installed. To fix the error, run the uninstall script with -s argument. This will add -refresh=false to terraform destroy command. How to deal with Error: Kubernetes cluster unreachable: the server has asked for the client to provide credentials error \u00b6 It is possible that you see such an error when running uninstall script with -s argument. If it's not possible to destroy infrastructure without it, delete the offending module from tfstate, for example: terraform state rm module.base-infrastructure.module.eks.helm_release.cluster-autoscaler Once done, re-run the uninstall script. How to deal with EIP AddressLimitExceeded error \u00b6 If you encounter the below error during installation stage, it means VPC is successfully created, but no Elastic IP addresses available. Error: Error creating EIP: AddressLimitExceeded: The maximum number of addresses has been reached. status code: 400 , request id: 0061b744-ced3-4d0e-9905-503c85013bcc with module.base-infrastructure.module.vpc.module.vpc.aws_eip.nat [ 0 ] , on .terraform/modules/base-infrastructure.vpc.vpc/main.tf line 1078 , in resource \"aws_eip\" \"nat\" : 1078 : resource \"aws_eip\" \"nat\" { It happens when an old VPC was deleted but associated Elastic IPs were not released. Refer to AWS documentation on how to release an Elastic IP address . Another option is to increase the Elastic UP address limit . How to deal with Nginx Ingress Helm deployment error \u00b6 If you encounter the below error when providing 25+ cidrs in whitelist_cidr variable, it may be caused by the service controller being unable to create a Load Balancer due to exceeding the inbound rules quota in a security group: module.base-infrastructure.module.ingress.helm_release.ingress: Still creating... [5m50s elapsed] Warning: Helm release \"ingress-nginx\" was created but has a failed status. Use the `helm` command to investigate the error, correct it, then run Terraform again. To check if it's really the case, login to the cluster and run: kubectl describe ingress-nginx-controller -n ingress-nginx to find the following error in Events section: Warning SyncLoadBalancerFailed 112s service-controller Error syncing load balancer: failed to ensure load balancer: error authorizing security group ingress: \"RulesPerSecurityGroupLimitExceeded: The maximum number of rules per security group has been reached.\\n\\tstatus code: 400, request id: 7de945ea-0571-48cd-99a1-c2ca528ad412\" The service controller creates several inbound rules for ports 80 and 443 for each source cidr, and as a result the quota is reached if there are 25+ cidrs in whitelist_cidr list. To mitigate the problem you may either file a ticket with AWS to increase the quota of inbound rules in a security group (60 by default) or set enable_https_ingress to false in config.tfvars if you don't need https ingresses. Port 443 will be removed from Nginx service, and as a result fewer inbound rules are created in the security group. With an increased inbound rules quota or enable_https_ingress set to false (or both), it is recommended to delete Nginx Helm chart before re-running install.sh : helm delete ingress-nginx -n ingress-nginx","title":"Troubleshooting tips"},{"location":"troubleshooting/TROUBLESHOOTING/#troubleshooting-tips","text":"This guide contains general tips on how to investigate an application deployment that doesn't work correctly. How to troubleshoot a failed Helm release installation?","title":"Troubleshooting tips"},{"location":"troubleshooting/TROUBLESHOOTING/#_1","text":"Symptom Install script fails due to failure when installing Helm release. This applies to all DC products. You will see the following error: module.confluence[0].helm_release.confluence: Still creating... [20m10s elapsed] Warning: Helm release \"confluence\" was created but has a failed status. Use the `helm` command to investigate the error, correct it, then run Terraform again. with module.confluence[0].helm_release.confluence, on modules/products/confluence/helm.tf line 4, in resource \"helm_release\" \"confluence\": 4: resource \"helm_release\" \"confluence\" { Error: timed out waiting for the condition with module.confluence[0].helm_release.confluence, on modules/products/confluence/helm.tf line 4, in resource \"helm_release\" \"confluence\": 4: resource \"helm_release\" \"confluence\" { Releasing state lock. This may take a few moments... Helm gives up waiting for a successful release, Usually, it means that Confluence (or any other product) pod failed to pass its readiness probe, or the pod is stuck in a Pending state. Solution To troubleshoot the error, run the following script: scripts/collect_k8s_logs.sh atlas-dcapt-confluence-small-cluster us-east-2 /path/to/local/directory Cluster name and region may differ (look at environment name and region in your config.tfvars ). For example, if your environment_name is dcapt-confluence-small , then your cluster name is atlas-dcapt-confluence-small-cluster . The last argument is a destination path for a tar.gz with logs that the script will produce. Share the archive in Slack #data-center-app-performance-toolkit channel along with your support request. You can also look at the pod and its logs, e.g.: confluence-0_log.log confluence-0_describe.log Odds are that logs may shed some light on why the pod isn't ready. The product_describe.log file will contain K8S events that may also help understand why the pod isn't in a Running state. It's also a good idea to get logs that are not sent to stdout/err: kubectl exec confluence-0 -n atlassian -i -t -- cat /var/atlassian/application-data/confluence/logs/atlassian-confluence.log Typically, if the pod is Running but not marked as Ready , it's the application that failed to start, i.e. it isn't an infrastructure issue. How to troubleshoot instances that failed to join the Kubernetes cluster","title":""},{"location":"troubleshooting/TROUBLESHOOTING/#_2","text":"Symptom When Terraform creates EKS infrastructure, EKS cluster (control plane) is created first. Once the cluster has been created, a NodeGroup (backed by an ASG) is created, and EC2 instances join the cluster as worker nodes. If a node fails to join its cluster, you will typically see the following error: Error: waiting for EKS Node Group (atlas-dcapt-jira-small-cluster:appNode-t3_xlarge-20240521085758213900000012) to create: unexpected state 'CREATE_FAILED', wanted target 'ACTIVE'. last error: 1 error occurred: * i-0dd1a9dc64303a10b: NodeCreationFailure: Instances failed to join the kubernetes cluster with module.base-infrastructure.module.eks.module.eks.module.eks_managed_node_group[\"appNodes\"].aws_eks_node_group.this[0], on .terraform/modules/base-infrastructure.eks.eks/modules/eks-managed-node-group/main.tf line 272, in resource \"aws_eks_node_group\" \"this\": 272: resource \"aws_eks_node_group\" \"this\" { There can be several reasons why nodes can't join the cluster. Permissions issues are the most common. Make sure STS is enabled for your account in the target region. With STS disabled, EKS control plane will deny join requests from the nodes. After enabling STS, destroy existing environment and re-run the installation. How to fix 'exec plugin is configured to use API version' error?","title":""},{"location":"troubleshooting/TROUBLESHOOTING/#_3","text":"Symptom When running install.sh script the installation fails with an error: module.base-infrastructure.kubernetes_namespace.products: Creating... module.base-infrastructure.module.ingress.helm_release.ingress: Creating... Error: Post \"https://1D2E0AC7AE5EC290740D816BD53A68AB.gr7.us-east-1.eks.amazonaws.com/api/v1/namespaces\": getting credentials: exec plugin is configured to use API version client.authentication.k8s.io/v1beta1, plugin returned version client.authentication.k8s.io/v1alpha1 with module.base-infrastructure.kubernetes_namespace.products, on modules/common/main.tf line 43, in resource \"kubernetes_namespace\" \"products\": 43: resource \"kubernetes_namespace\" \"products\" { Error: Kubernetes cluster unreachable: Get \"https://1D2E0AC7AE5EC290740D816BD53A68AB.gr7.us-east-1.eks.amazonaws.com/version\": getting credentials: decoding stdout: no kind \"ExecCredential\" is registered for version \"client.authentication.k8s.io/v1alpha1\" in scheme \"pkg/runtime/scheme.go:100\" with module.base-infrastructure.module.ingress.helm_release.ingress, on modules/AWS/ingress/main.tf line 44, in resource \"helm_release\" \"ingress\": 44: resource \"helm_release\" \"ingress\" { The error is caused by API version mismatch. AWS CLI and Kubernetes and Helm providers use v1alpha1 and v1beta1 respectively. Solution Update AWS CLI to the most recent version. How do I uninstall an environment using a different Terraform configuration file?","title":""},{"location":"troubleshooting/TROUBLESHOOTING/#_4","text":"Symptom If you try to uninstall an environment by using a different configuration file than the one you used to install it or by using a different version of the code, you may encounter some issues during uninstallation. In most cases, Terraform reports that the resource cannot be removed because it's in use. Solution Identify the resource and delete it manually from the AWS console, and then restart the uninstallation process. Make sure to always use the same configuration file that was used to install the environment. How do I deal with persistent volumes that do not get removed as part of the Terraform uninstallation?","title":""},{"location":"troubleshooting/TROUBLESHOOTING/#_5","text":"Symptom Uninstall fails to remove the persistent volume. Error: Persistent volume atlassian-dc-share-home-pv still exists ( Bound ) Error: context deadline exceeded Error: Persistent volume claim atlassian-dc-share-home-pvc still exists with Solution If a pod termination stalls, it will block pvc and pv deletion. To fix this problem we need to terminate product pod first and run uninstall command again. kubectl delete pod <stalled-pod> -n atlassian --force To see the stalled pod name you can run the following command: kubectl get pods -n atlassian How do I deal with suspended AWS Auto Scaling Groups during Terraform uninstallation?","title":""},{"location":"troubleshooting/TROUBLESHOOTING/#_6","text":"Symptom If for any reason Auto Scaling Group gets suspended, AWS does not allow Terraform to delete the node group. In cases like this the uninstall process gets interrupted with the following error: Error: error waiting for EKS Node Group ( atlas-<environment_name>-cluster:appNode ) to delete: unexpected state 'DELETE_FAILED' , wanted target '' . last error: 2 errors occurred: * i-06a4b4afc9e7a76b0: NodeCreationFailure: Instances failed to join the kubernetes cluster * eks-appNode-3ebedddc-2d97-ff10-6c23-4900d1d79599: AutoScalingGroupInvalidConfiguration: Couldn ' t terminate instances in ASG as Terminate process is suspended Solution Delete the reported Auto Scaling Group in AWS console and run uninstall command again. How do I deal with Terraform AWS authentication issues during installation?","title":""},{"location":"troubleshooting/TROUBLESHOOTING/#_7","text":"Symptom The following error is thrown: An error occurred ( ExpiredToken ) when calling the GetCallerIdentity operation: The security token included in the request is expired Solution Terraform cannot deploy resources to AWS if your security token has expired. Renew your token and retry. How do I deal with Terraform state lock acquisition errors?","title":""},{"location":"troubleshooting/TROUBLESHOOTING/#_8","text":"If user interrupts the installation or uninstallation process, Terraform won't be able to unlock resources. In this case, Terraform is unable to acquire state lock in the next attempt. Symptom The following error is thrown: Acquiring state lock. This may take a few moments... Error: Error acquiring the state lock Error message: ConditionalCheckFailedException: The conditional request failed Lock Info: ID: 26f7b9a8-1bef-0674-669b-1d60800dea4d Path: atlassian-data-center-terraform-state-xxxxxxxxxx/bamboo-xxxxxxxxxx/terraform.tfstate Operation: OperationTypeApply Who: xxxxxxxxxx@C02CK0JYMD6V Version: 1 .0.9 Created: 2021 -11-04 00 :50:34.736134 +0000 UTC Info: Solution Forcibly unlock the state by running the following command: terraform force-unlock <ID> Where <ID> is the value that appears in the error message. There are two Terraform locks; one for the infrastructure and another for Terraform state. If you are still experiencing lock issues, change the directory to ./modules/tfstate and retry the same command. How do I deal with state data in S3 that does not have the expected content?","title":""},{"location":"troubleshooting/TROUBLESHOOTING/#_9","text":"If Terraform state is locked and users forcefully unlock it using terraform force-unlock <id> , it may not get a chance to update the Digest value in DynamoDB. This prevents Terraform from reading the state data. Symptom The following error is thrown: Error refreshing state: state data in S3 does not have the expected content. This may be caused by unusually long delays in S3 processing a previous state update. Please wait for a minute or two and try again. If this problem persists, and neither S3 nor DynamoDB are experiencing an outage, you may need to manually verify the remote state and update the Digest value stored in the DynamoDB table to the following value: 531ca9bce76bbe0262f610cfc27bbf0b Solution Open DynamoDB page in AWS console and find the table named atlassian_data_center_<region>_<aws_account_id>_tf_lock in the same region as the cluster. Click on Explore Table Items and find the LockID named <table_name>/<environment_name>/terraform.tfstate-md5 . Click on the item and replace the Digest value with the given value in the error message. How do I deal with pre-existing state in multiple environment?","title":""},{"location":"troubleshooting/TROUBLESHOOTING/#_10","text":"If you start installing a new environment while you already have an active environment installed before, you should NOT use the pre-existing state. The same scenario when you want to uninstall a non-active environment. What is active environment? Active environment is the latest environment you installed or uninstalled. Tip Answer ' NO ' when you get a similar message during installation or uninstallation: Do you want to copy existing state to the new backend? Pre-existing state was found while migrating the previous \"s3\" backend to the newly configured \"s3\" backend. An existing non-empty state already exists in the new backend. The two states have been saved to temporary files that will be removed after responding to this query. Do you want to overwrite the state in the new backend with the previous state? Enter \"yes\" to copy and \"no\" to start with the existing state in the newly configured \"s3\" backend. Enter a value: Symptom Installation or uninstallation break after you chose to use pre-existing state. Solution Clean up the project before proceed. In root directory of the project run: ./scripts/cleanup.sh -s -t -x -r . terraform init -var-file = <config file> Then re-run the install/uninstall script. How do I deal with Module not installed error during uninstallation?","title":""},{"location":"troubleshooting/TROUBLESHOOTING/#_11","text":"There are some Terraform specific modules that are required when performing an uninstall. These modules are generated by Terraform during the install process and are stored in the .terraform folder. If Terraform cannot find these modules, then it won't be able perform an uninstall of the infrastructure. Symptom Error: Module not installed on main.tf line 7 : 7 : module \"tfstate-bucket\" { This module is not yet installed. Run \"terraform init\" to install all modules required by this configuration. Solution In the root directory of the project run: ./scripts/cleanup.sh -s -t -x -r . cd modules/tfstate terraform init -var-file = <config file> Go back to the root of the project and re-run the uninstall.sh script. How to deal with getting credentials: exec: executable aws failed with exit code 2 error?","title":""},{"location":"troubleshooting/TROUBLESHOOTING/#_12","text":"Symptom After performing an install.sh the following error is encountered: Error: Post \"https://0839E580E6ADB7B784AECE0E152D8AF2.gr7.eu-west-1.eks.amazonaws.com/api/v1/namespaces\" : getting credentials: exec: executable aws failed with exit code 2 with module.base-infrastructure.kubernetes_namespace.products, on modules/common/main.tf line 39 , in resource \"kubernetes_namespace\" \"products\" : 39 : resource \"kubernetes_namespace\" \"products\" { Solution Ensure you are using a version of the aws cli that is at least >= 2 The version can be checked by running: aws --version How to ssh to application nodes?","title":""},{"location":"troubleshooting/TROUBLESHOOTING/#_13","text":"Sometimes you need to ssh to the application nodes. This can be done by running: kubectl exec -it <pod-name> -n atlassian -- /bin/bash where <pod-name> is the name of the application pod you want to ssh such as bitbucket-0 or jira-1 . To get the pod names you can run: kubectl get pods -n atlassian How to access to the application log files?","title":""},{"location":"troubleshooting/TROUBLESHOOTING/#_14","text":"A simple way to access to the application log content is running the following command: kubectl logs <pod-name> -n atlassian where <pod-name> is the name of the application pod you want to see the log such as bitbucket-0 or jira-1 . To get the pod names you can run: kubectl get pods -n atlassian However, another approach to see the full log files produced by the application would be to ssh to the application pod and access directly the log folder. kubectl exec -it <pod-name> -n atlassian -- /bin/bash cd /var/atlassian/<application>/logs where <application> is the name of the application such as confluence , bamboo , bitbucket , or jira . Note that for some applications log foler is /var/atlassian/<application>/log and for others is /var/atlassian/<application>/logs . If you need to copy the log files to a local machine, you can use the following command: kubectl cp atlassian/<pod-name>:/var/atlassian/<application>/logs/<log_files> <local-path> How to deal with persistent volume claim destroy failed error?","title":""},{"location":"troubleshooting/TROUBLESHOOTING/#_15","text":"The PVC cannot be destroyed when bound to a pod. Overcome this by scaling down to 0 pods first before deleting PVC. helm upgrade PRODUCT atlassian-data-center/PRODUCT --set replicaCount=0 --reuse-values -n atlassian How to manually clean up resources when uninstall has failed?","title":""},{"location":"troubleshooting/TROUBLESHOOTING/#_16","text":"Sometimes Terraform is unable to destroy resources for various reasons. This normally happens at EKS level. One quick solution is to manually delete the EKS cluster, and re-run uninstall, so that Terraform will pick up from there. To delete EKS cluster, go to AWS console > EKS service > the cluster you're deploying. You'll need to go to 'Configuration' tab > 'Compute' tab > click into node group. Then in node group screen > Details > click into Autoscaling group. It'll then direct you to EC2 > Auto Scaling Group screen with the ASG selected > 'Delete' the chosen ASG. Wait for the ASG to be deleted, then go back to EKS cluster > Delete. How to deal with This object does not have an attribute named error when running uninstall.sh","title":""},{"location":"troubleshooting/TROUBLESHOOTING/#_17","text":"It is possible that if the installation has failed, the uninstall script will return an error like: module.base-infrastructure.module.eks.aws_autoscaling_group_tag.this[\"Name\"]: Refreshing state... [id=eks-appNode-t3_xlarge-50c26268-ea57-5aee-4523-68f33af7dd71,Name] Error: Unsupported attribute on dc-infrastructure.tf line 142, in module \"confluence\": 142: ingress = module.base-infrastructure.ingress \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2502 module.base-infrastructure is object with 5 attributes This object does not have an attribute named \"ingress\". Error: Unsupported attribute This happens because some of the modules failed to be installed. To fix the error, run the uninstall script with -s argument. This will add -refresh=false to terraform destroy command. How to deal with Error: Kubernetes cluster unreachable: the server has asked for the client to provide credentials error","title":""},{"location":"troubleshooting/TROUBLESHOOTING/#_18","text":"It is possible that you see such an error when running uninstall script with -s argument. If it's not possible to destroy infrastructure without it, delete the offending module from tfstate, for example: terraform state rm module.base-infrastructure.module.eks.helm_release.cluster-autoscaler Once done, re-run the uninstall script. How to deal with EIP AddressLimitExceeded error","title":""},{"location":"troubleshooting/TROUBLESHOOTING/#_19","text":"If you encounter the below error during installation stage, it means VPC is successfully created, but no Elastic IP addresses available. Error: Error creating EIP: AddressLimitExceeded: The maximum number of addresses has been reached. status code: 400 , request id: 0061b744-ced3-4d0e-9905-503c85013bcc with module.base-infrastructure.module.vpc.module.vpc.aws_eip.nat [ 0 ] , on .terraform/modules/base-infrastructure.vpc.vpc/main.tf line 1078 , in resource \"aws_eip\" \"nat\" : 1078 : resource \"aws_eip\" \"nat\" { It happens when an old VPC was deleted but associated Elastic IPs were not released. Refer to AWS documentation on how to release an Elastic IP address . Another option is to increase the Elastic UP address limit . How to deal with Nginx Ingress Helm deployment error","title":""},{"location":"troubleshooting/TROUBLESHOOTING/#_20","text":"If you encounter the below error when providing 25+ cidrs in whitelist_cidr variable, it may be caused by the service controller being unable to create a Load Balancer due to exceeding the inbound rules quota in a security group: module.base-infrastructure.module.ingress.helm_release.ingress: Still creating... [5m50s elapsed] Warning: Helm release \"ingress-nginx\" was created but has a failed status. Use the `helm` command to investigate the error, correct it, then run Terraform again. To check if it's really the case, login to the cluster and run: kubectl describe ingress-nginx-controller -n ingress-nginx to find the following error in Events section: Warning SyncLoadBalancerFailed 112s service-controller Error syncing load balancer: failed to ensure load balancer: error authorizing security group ingress: \"RulesPerSecurityGroupLimitExceeded: The maximum number of rules per security group has been reached.\\n\\tstatus code: 400, request id: 7de945ea-0571-48cd-99a1-c2ca528ad412\" The service controller creates several inbound rules for ports 80 and 443 for each source cidr, and as a result the quota is reached if there are 25+ cidrs in whitelist_cidr list. To mitigate the problem you may either file a ticket with AWS to increase the quota of inbound rules in a security group (60 by default) or set enable_https_ingress to false in config.tfvars if you don't need https ingresses. Port 443 will be removed from Nginx service, and as a result fewer inbound rules are created in the security group. With an increased inbound rules quota or enable_https_ingress set to false (or both), it is recommended to delete Nginx Helm chart before re-running install.sh : helm delete ingress-nginx -n ingress-nginx","title":""},{"location":"userguide/CLEANUP/","text":"Uninstallation and Cleanup \u00b6 This guide describes how to uninstall all Atlassian Data Center products and remove cloud environments Do you want to install a DC product but still keep the common infrastructure and other installed products? To uninstall one or more products without destroying the infrastructure, remove the product names from environment's config file and re-run install command. The uninstallation process is destructive The uninstallation process will permanently delete the local volume, shared volume, and the database. Terraform state information can also optionally be removed. Before you begin, make sure that you have an up-to-date backup available in a secure location. The uninstallation script is located in the root folder of the project directory. Usage: ./uninstall.sh [ -c <config_file> ] [ -h ] [ -f ] [ -s ] \" The following options are available: -c <config_file_path> - Pass a custom configuration file to uninstall the environment provisioned by it. -f - skip manual confirmation of the environment deletion.\" -s - skip refresh when running terraform destroy\" -h - provides help to how executing this script.\" Uninstallation using default and custom configuration files If you used the default configuration file ( config.tfvars ) from the root folder of the project, run the following command: ./uninstall.sh Alternatively if you used a custom configuration file to provision the infrastructure, run the following command using the same configuration file: ./uninstall.sh -c my-custom-config.tfvars","title":"Uninstallation"},{"location":"userguide/CLEANUP/#uninstallation-and-cleanup","text":"This guide describes how to uninstall all Atlassian Data Center products and remove cloud environments Do you want to install a DC product but still keep the common infrastructure and other installed products? To uninstall one or more products without destroying the infrastructure, remove the product names from environment's config file and re-run install command. The uninstallation process is destructive The uninstallation process will permanently delete the local volume, shared volume, and the database. Terraform state information can also optionally be removed. Before you begin, make sure that you have an up-to-date backup available in a secure location. The uninstallation script is located in the root folder of the project directory. Usage: ./uninstall.sh [ -c <config_file> ] [ -h ] [ -f ] [ -s ] \" The following options are available: -c <config_file_path> - Pass a custom configuration file to uninstall the environment provisioned by it. -f - skip manual confirmation of the environment deletion.\" -s - skip refresh when running terraform destroy\" -h - provides help to how executing this script.\" Uninstallation using default and custom configuration files If you used the default configuration file ( config.tfvars ) from the root folder of the project, run the following command: ./uninstall.sh Alternatively if you used a custom configuration file to provision the infrastructure, run the following command using the same configuration file: ./uninstall.sh -c my-custom-config.tfvars","title":"Uninstallation and Cleanup"},{"location":"userguide/INSTALLATION/","text":"Installation \u00b6 This guide describes how to provision the cloud environment infrastructure and install Atlassian Data Center products in a Kubernetes cluster running on AWS. 1. Set up AWS security credentials \u00b6 Set up a user with an administrator IAM role. See Configuration basics \u2014 AWS Command Line Interface . 2. Clone the project repository \u00b6 Clone the data-center-terraform project repository from GitHub: git clone -b 2 .9.10 https://github.com/atlassian-labs/data-center-terraform.git && cd data-center-terraform 3. Configure the infrastructure \u00b6 Details of the desired infrastructure to be provisioned can be defined in config.tfvars located in the root level of the cloned project. Additional details on how this file can/should be configured can be found in the Configuration guide . Configuration file location? By default, Terraform uses config.tfvars located in the root level of the project. Can I use a custom configuration file? You can use a custom configuration file, but it must follow the same format as the default configuration file. You can make a copy of config.tfvars , renaming the copy and using config.tfvars as a template to define your own infrastructure configuration. How to install more than one DC product? More than one DC products can be provisioned to the same cluster. See the Configuration guide for more details. You can also install DC products to an existing environment by adding the product in the environment's config file and re-run the install command. Use the same configuration file for uninstallation and cleanup If you have more than one environment, make sure to manage the configuration file for each environment separately. When cleaning up your environment, use the same configuration file that was used to create it originally. 4. Run the installation script \u00b6 Based on how config.tfvars has been configured the installation script will Provision the environment and infrastructure Install the selected DC product(s) Installation is fully automated and requires no user intervention. Terraform is invoked under the hood which handles the creation and management of the Kubernetes infrastructure. Terraform deployment details To keep track of the current state of the resources and manage changes, Terraform creates an AWS S3 bucket to store the current state of the environment. An AWS DynamoDB table is created to handle the locking of remote state files during the installation, upgrade, and cleanup stages to prevent the environment from being modified by more than one process at a time. The installation script, install.sh , is located in the root folder of the project. Usage: ./install.sh [ -c <config_file_path> ] [ -h ] The following options are available: -c <config_file_path> - Pass a custom configuration file when provisioning multiple environments -h - Display help information Using the same cloned repository to manage more than one environment If the repository has already been used to deploy an environment, and that environment is still active, i.e. hasn't been uninstalled yet, deploying a new environment using install.sh will get a prompt with following message: Do you want to copy existing state to the new backend? Pre-existing state was found while migrating the previous \"s3\" backend to the newly configured \"s3\" backend. An existing non-empty state already exists in the new backend. The two states have been saved to temporary files that will be removed after responding to this query. Previous ( type \"s3\" ) : /var/folders/vm/sz46pmw94f3f8nrvzyqhwmx00000gn/T/terraform3661306827/1-s3.tfstate New ( type \"s3\" ) : /var/folders/vm/sz46pmw94f3f8nrvzyqhwmx00000gn/T/terraform3661306827/2-s3.tfstate Do you want to overwrite the state in the new backend with the previous state? Enter \"yes\" to copy and \"no\" to start with the existing state in the newly configured \"s3\" backend. Enter a value: This will happen everytime when you switch between different active environments. Since environemnts are independent, answer ' NO ' to continue. If you answered Yes, Terraform will then use the state of active environment to plan and deploy new environment, which will pollute the state of both environments, and end up to an error state. Check troubleshoting guide if you accidentally answered Yes. Running the installation script with no parameters will use the default configuration file to provision the environment. Installation using default and custom configuration files Running the installation script with no parameters will use the default configuration file ( config.tfvars ) to provision the environment: ./install.sh Alternatively a custom configuration file can be specified as follows: ./install.sh -c my-custom-config.tfvars How do I find the service URL of the deployed DC product? When the installation process finishes successfully detailed information about the infrastructure is printed to STDOUT , this includes the product_urls value that can be used to launch the product in the browser. Where do I find the database username and password ? The database master username and password for each product is dynamically generated by Terraform and saved in a Kubernetes secret within the product namespace . To access the database username and password, run the following commands: DB_SECRETS=$(kubectl get secret <product-name>-db-cred -n atlassian -o jsonpath='{.data}') DB_USERNAME=$(echo $DB_SECRETS | jq -r '.username' | base64 --decode) DB_PASSWORD=$(echo $DB_SECRETS | jq -r '.password' | base64 --decode) This saves the decoded username and password to the $DB_USERNAME and $DB_PASSWORD environment variables respectively. Uninstall \u00b6 The deployment and all of its associated resources can be un-installed by following the Uninstallation and cleanup guide.","title":"Installation"},{"location":"userguide/INSTALLATION/#installation","text":"This guide describes how to provision the cloud environment infrastructure and install Atlassian Data Center products in a Kubernetes cluster running on AWS.","title":"Installation"},{"location":"userguide/INSTALLATION/#1-set-up-aws-security-credentials","text":"Set up a user with an administrator IAM role. See Configuration basics \u2014 AWS Command Line Interface .","title":"1. Set up AWS security credentials"},{"location":"userguide/INSTALLATION/#2-clone-the-project-repository","text":"Clone the data-center-terraform project repository from GitHub: git clone -b 2 .9.10 https://github.com/atlassian-labs/data-center-terraform.git && cd data-center-terraform","title":"2. Clone the project repository"},{"location":"userguide/INSTALLATION/#3-configure-the-infrastructure","text":"Details of the desired infrastructure to be provisioned can be defined in config.tfvars located in the root level of the cloned project. Additional details on how this file can/should be configured can be found in the Configuration guide . Configuration file location? By default, Terraform uses config.tfvars located in the root level of the project. Can I use a custom configuration file? You can use a custom configuration file, but it must follow the same format as the default configuration file. You can make a copy of config.tfvars , renaming the copy and using config.tfvars as a template to define your own infrastructure configuration. How to install more than one DC product? More than one DC products can be provisioned to the same cluster. See the Configuration guide for more details. You can also install DC products to an existing environment by adding the product in the environment's config file and re-run the install command. Use the same configuration file for uninstallation and cleanup If you have more than one environment, make sure to manage the configuration file for each environment separately. When cleaning up your environment, use the same configuration file that was used to create it originally.","title":"3. Configure the infrastructure"},{"location":"userguide/INSTALLATION/#4-run-the-installation-script","text":"Based on how config.tfvars has been configured the installation script will Provision the environment and infrastructure Install the selected DC product(s) Installation is fully automated and requires no user intervention. Terraform is invoked under the hood which handles the creation and management of the Kubernetes infrastructure. Terraform deployment details To keep track of the current state of the resources and manage changes, Terraform creates an AWS S3 bucket to store the current state of the environment. An AWS DynamoDB table is created to handle the locking of remote state files during the installation, upgrade, and cleanup stages to prevent the environment from being modified by more than one process at a time. The installation script, install.sh , is located in the root folder of the project. Usage: ./install.sh [ -c <config_file_path> ] [ -h ] The following options are available: -c <config_file_path> - Pass a custom configuration file when provisioning multiple environments -h - Display help information Using the same cloned repository to manage more than one environment If the repository has already been used to deploy an environment, and that environment is still active, i.e. hasn't been uninstalled yet, deploying a new environment using install.sh will get a prompt with following message: Do you want to copy existing state to the new backend? Pre-existing state was found while migrating the previous \"s3\" backend to the newly configured \"s3\" backend. An existing non-empty state already exists in the new backend. The two states have been saved to temporary files that will be removed after responding to this query. Previous ( type \"s3\" ) : /var/folders/vm/sz46pmw94f3f8nrvzyqhwmx00000gn/T/terraform3661306827/1-s3.tfstate New ( type \"s3\" ) : /var/folders/vm/sz46pmw94f3f8nrvzyqhwmx00000gn/T/terraform3661306827/2-s3.tfstate Do you want to overwrite the state in the new backend with the previous state? Enter \"yes\" to copy and \"no\" to start with the existing state in the newly configured \"s3\" backend. Enter a value: This will happen everytime when you switch between different active environments. Since environemnts are independent, answer ' NO ' to continue. If you answered Yes, Terraform will then use the state of active environment to plan and deploy new environment, which will pollute the state of both environments, and end up to an error state. Check troubleshoting guide if you accidentally answered Yes. Running the installation script with no parameters will use the default configuration file to provision the environment. Installation using default and custom configuration files Running the installation script with no parameters will use the default configuration file ( config.tfvars ) to provision the environment: ./install.sh Alternatively a custom configuration file can be specified as follows: ./install.sh -c my-custom-config.tfvars How do I find the service URL of the deployed DC product? When the installation process finishes successfully detailed information about the infrastructure is printed to STDOUT , this includes the product_urls value that can be used to launch the product in the browser. Where do I find the database username and password ? The database master username and password for each product is dynamically generated by Terraform and saved in a Kubernetes secret within the product namespace . To access the database username and password, run the following commands: DB_SECRETS=$(kubectl get secret <product-name>-db-cred -n atlassian -o jsonpath='{.data}') DB_USERNAME=$(echo $DB_SECRETS | jq -r '.username' | base64 --decode) DB_PASSWORD=$(echo $DB_SECRETS | jq -r '.password' | base64 --decode) This saves the decoded username and password to the $DB_USERNAME and $DB_PASSWORD environment variables respectively.","title":"4. Run the installation script"},{"location":"userguide/INSTALLATION/#uninstall","text":"The deployment and all of its associated resources can be un-installed by following the Uninstallation and cleanup guide.","title":"Uninstall"},{"location":"userguide/PREREQUISITES/","text":"Prerequisites \u00b6 Before installing the infrastructure for Atlassian Data Center products, make sure that you meet the following requirements and that your local environment is configured with all the necessary tools. Environment setup \u00b6 Its advised that the tooling below is installed to your development environment. A basic understanding of these tools and their associated concepts is also advisable. Terraform Helm v3.3 or later AWS CLI v2.7 or later Kubectl (optional) Kubernetes cluster monitoring tools (optional) Terraform \u00b6 Terraform is an open-source infrastructure as code tool that provides a consistent CLI workflow to create and manage the infrastructure of cloud environments. This project uses Terraform to create and manage the Atlassian Data Center infrastructure on AWS for use with supported Data Center products. Check if Terraform is already installed by running the following command: terraform version If Terraform is not installed, install it by following the official instructions . Helm \u00b6 Atlassian supports Helm Charts for its Data Center products . This project uses the Data Center Helm charts to package Atlassian DC products as a turnkey solution for your cloud infrastructure. Before using this project, make sure that Helm v3.3 or later is installed on your machine. Check if Helm v3.3 or later is already installed by running the following command: helm version --short If Helm is not installed or you're running a version lower than 3.3, install Helm by following the official instructions . AWS CLI \u00b6 You need to have the AWS CLI tool installed on your local machine before creating the Kubernetes infrastructure. AWS CLI Version 2 Version 2 of the AWS CLI is required. Check if AWS CLI version 2 is already installed by running the following command: aws --version If the AWS CLI is not installed, or you're running version 1, install AWS CLI version 2 by following the official instructions . Kubectl (optional) \u00b6 Kubectl is a command line tool lets you control Kubernetes clusters. Check if kubectl is already installed by running the following command: kubectl version If not installed this can be done by following the official instructions . Kubernetes cluster monitoring tools \u00b6 Kubernetes monitoring and issue diagnosis can be facilitated with a monitoring tool like one of those listed below. Installation and usage of one is not a requirement for deployments with this project but can certainly be of help when problems arise. Kubernetes monitoring tools Prometheus Grafana Weave Scope","title":"Prerequisites"},{"location":"userguide/PREREQUISITES/#prerequisites","text":"Before installing the infrastructure for Atlassian Data Center products, make sure that you meet the following requirements and that your local environment is configured with all the necessary tools.","title":"Prerequisites"},{"location":"userguide/PREREQUISITES/#environment-setup","text":"Its advised that the tooling below is installed to your development environment. A basic understanding of these tools and their associated concepts is also advisable. Terraform Helm v3.3 or later AWS CLI v2.7 or later Kubectl (optional) Kubernetes cluster monitoring tools (optional)","title":"Environment setup"},{"location":"userguide/PREREQUISITES/#terraform","text":"Terraform is an open-source infrastructure as code tool that provides a consistent CLI workflow to create and manage the infrastructure of cloud environments. This project uses Terraform to create and manage the Atlassian Data Center infrastructure on AWS for use with supported Data Center products. Check if Terraform is already installed by running the following command: terraform version If Terraform is not installed, install it by following the official instructions .","title":"Terraform"},{"location":"userguide/PREREQUISITES/#helm","text":"Atlassian supports Helm Charts for its Data Center products . This project uses the Data Center Helm charts to package Atlassian DC products as a turnkey solution for your cloud infrastructure. Before using this project, make sure that Helm v3.3 or later is installed on your machine. Check if Helm v3.3 or later is already installed by running the following command: helm version --short If Helm is not installed or you're running a version lower than 3.3, install Helm by following the official instructions .","title":"Helm"},{"location":"userguide/PREREQUISITES/#aws-cli","text":"You need to have the AWS CLI tool installed on your local machine before creating the Kubernetes infrastructure. AWS CLI Version 2 Version 2 of the AWS CLI is required. Check if AWS CLI version 2 is already installed by running the following command: aws --version If the AWS CLI is not installed, or you're running version 1, install AWS CLI version 2 by following the official instructions .","title":"AWS CLI"},{"location":"userguide/PREREQUISITES/#kubectl-optional","text":"Kubectl is a command line tool lets you control Kubernetes clusters. Check if kubectl is already installed by running the following command: kubectl version If not installed this can be done by following the official instructions .","title":"Kubectl (optional)"},{"location":"userguide/PREREQUISITES/#kubernetes-cluster-monitoring-tools","text":"Kubernetes monitoring and issue diagnosis can be facilitated with a monitoring tool like one of those listed below. Installation and usage of one is not a requirement for deployments with this project but can certainly be of help when problems arise. Kubernetes monitoring tools Prometheus Grafana Weave Scope","title":"Kubernetes cluster monitoring tools"},{"location":"userguide/configuration/BAMBOO_CONFIGURATION/","text":"Bamboo configuration \u00b6 Application configuration \u00b6 Helm chart version \u00b6 bamboo_helm_chart_version sets the Helm chart version of Bamboo instance. bamboo_helm_chart_version = \"1.2.0\" Bamboo version tag \u00b6 Bamboo will be installed with the default version defined in its Helm chart . If you want to install a specific version of Bamboo, you can set the bamboo_version_tag to the version you want to install. For more information, see Bamboo Version Tags . bamboo_version_tag = \"<BAMBOO_VERSION_TAG>\" Installation timeout \u00b6 bamboo_installation_timeout defines the timeout (in minutes) for product helm chart installation . Different variables can influence how long it takes the application from installation to ready state. These can be dataset restoration, resource requirements, number of replicas and others. bamboo_installation_timeout = 10 License \u00b6 bamboo_license takes the license key of Bamboo product. Make sure that there is no new lines or spaces in license key. bamboo_license = \"<LICENSE_KEY>\" Sensitive data bamboo_license is marked as sensitive, storing in a plain-text config.tfvars file is not recommended. Please refer to Sensitive Data section. System Admin Credentials \u00b6 Four values are required to configure Bamboo system admin credentials. bamboo_admin_username = \"<USERNAME>\" bamboo_admin_password = \"<PASSWORD>\" bamboo_admin_display_name = \"<DISPLAY_NAME>\" bamboo_admin_email_address = \"<EMAIL_ADDRESS>\" Sensitive data bamboo_admin_password is marked as sensitive, storing in a plain-text config.tfvars file is not recommended. Please refer to Sensitive Data section. Restoring from existing dataset If the dataset_url variable is provided (see Restoring from Backup below), the Bamboo System Admin Credentials properties are ignored. You will need to use user credentials from the dataset to log into the instance. Instance resource configuration \u00b6 The following variables set number of CPU, amount of memory, maximum heap size and minimum heap size of Bamboo instance. (Used default values as example.) bamboo_cpu = \"1\" bamboo_mem = \"1Gi\" bamboo_min_heap = \"256m\" bamboo_max_heap = \"512m\" Bamboo Agents configuration \u00b6 Agent Helm chart version \u00b6 bamboo_helm_chart_version sets the Helm chart version of Bamboo Agent instance. bamboo_agent_helm_chart_version = \"1.2.0\" Agent version tag \u00b6 Bamboo Agent will be installed with the default version defined in its Helm chart . If you want to install a specific version of the Agent, you can set the bamboo_agent_version_tag to the desired version. For more information, see Bamboo Agent Version Tags . bamboo_agent_version_tag = \"<BAMBOO_AGENT_VERSION_TAG>\" Agent instance resource configuration \u00b6 The following variables set number of CPU and amount of memory of Bamboo Agent instances. (Used default values as example.) bamboo_agent_cpu = \"0.25\" bamboo_agent_mem = \"256m\" Number of agents \u00b6 number_of_bamboo_agents sets the number of remote agents to be launched. To disable agents, set this value to 0 . number_of_bamboo_agents = 5 The number of agents is limited to the number of allowed agents in your license. Any agents beyond the allowed number won't be able to join the cluster. A valid license is required to install bamboo agents Bamboo needs a valid license to install remote agents. Disable agents if you don't provide a license at installation time. RDS Configuration \u00b6 Database engine version \u00b6 bamboo_db_major_engine_version sets the PostgeSQL engine version that will be used. bamboo_db_major_engine_version = \"13\" Supported DB versions Be sure to use a DB engine version that is supported by Bamboo Database Instance Class \u00b6 bamboo_db_instance_class sets the DB instance type that allocates the computational, network, and memory capacity required by the planned workload of the DB instance. For more information about available instance classes, see DB instance classes \u2014 Amazon Relational Database Service . bamboo_db_instance_class = \"<INSTANCE_CLASS>\" # e.g. \"db.t3.micro\" Database Allocated Storage \u00b6 bamboo_db_allocated_storage sets the allocated storage for the database instance in GiB. bamboo_db_allocated_storage = 100 The allowed value range of allocated storage may vary based on instance class You may want to adjust these values according to your needs. For more information, see Amazon RDS DB instance storage \u2014 Amazon Relational Database Service . Database IOPS \u00b6 bamboo_db_iops sets the requested number of I/O operations per second that the DB instance can support. bamboo_db_iops = 1000 The allowed value range of IOPS may vary based on instance class You may want to adjust these values according to your needs. For more information, see Amazon RDS DB instance storage \u2014 Amazon Relational Database Service . Database name \u00b6 bamboo_db_name defines the name of database to be used for the Bamboo in RDS instance. bamboo_db_name = \"bamboo\" Restoring from Backup \u00b6 To restore data from an existing Bamboo backup , you can set the dataset_url variable to a publicly accessible URL where the dataset can be downloaded. dataset_url = \"https://bamboo-test-datasets.s3.amazonaws.com/dcapt-bamboo-no-agents.zip\" This dataset is downloaded to the shared home and then imported by the Bamboo instance. To log in to the instance, you will need to use any credentials from the dataset. Provisioning time Restoring from the dataset will increase the time it takes to create the environment.","title":"Bamboo configuration"},{"location":"userguide/configuration/BAMBOO_CONFIGURATION/#bamboo-configuration","text":"","title":"Bamboo configuration"},{"location":"userguide/configuration/BAMBOO_CONFIGURATION/#application-configuration","text":"","title":"Application configuration"},{"location":"userguide/configuration/BAMBOO_CONFIGURATION/#helm-chart-version","text":"bamboo_helm_chart_version sets the Helm chart version of Bamboo instance. bamboo_helm_chart_version = \"1.2.0\"","title":"Helm chart version"},{"location":"userguide/configuration/BAMBOO_CONFIGURATION/#bamboo-version-tag","text":"Bamboo will be installed with the default version defined in its Helm chart . If you want to install a specific version of Bamboo, you can set the bamboo_version_tag to the version you want to install. For more information, see Bamboo Version Tags . bamboo_version_tag = \"<BAMBOO_VERSION_TAG>\"","title":"Bamboo version tag"},{"location":"userguide/configuration/BAMBOO_CONFIGURATION/#installation-timeout","text":"bamboo_installation_timeout defines the timeout (in minutes) for product helm chart installation . Different variables can influence how long it takes the application from installation to ready state. These can be dataset restoration, resource requirements, number of replicas and others. bamboo_installation_timeout = 10","title":"Installation timeout"},{"location":"userguide/configuration/BAMBOO_CONFIGURATION/#license","text":"bamboo_license takes the license key of Bamboo product. Make sure that there is no new lines or spaces in license key. bamboo_license = \"<LICENSE_KEY>\" Sensitive data bamboo_license is marked as sensitive, storing in a plain-text config.tfvars file is not recommended. Please refer to Sensitive Data section.","title":"License"},{"location":"userguide/configuration/BAMBOO_CONFIGURATION/#system-admin-credentials","text":"Four values are required to configure Bamboo system admin credentials. bamboo_admin_username = \"<USERNAME>\" bamboo_admin_password = \"<PASSWORD>\" bamboo_admin_display_name = \"<DISPLAY_NAME>\" bamboo_admin_email_address = \"<EMAIL_ADDRESS>\" Sensitive data bamboo_admin_password is marked as sensitive, storing in a plain-text config.tfvars file is not recommended. Please refer to Sensitive Data section. Restoring from existing dataset If the dataset_url variable is provided (see Restoring from Backup below), the Bamboo System Admin Credentials properties are ignored. You will need to use user credentials from the dataset to log into the instance.","title":"System Admin Credentials"},{"location":"userguide/configuration/BAMBOO_CONFIGURATION/#instance-resource-configuration","text":"The following variables set number of CPU, amount of memory, maximum heap size and minimum heap size of Bamboo instance. (Used default values as example.) bamboo_cpu = \"1\" bamboo_mem = \"1Gi\" bamboo_min_heap = \"256m\" bamboo_max_heap = \"512m\"","title":"Instance resource configuration"},{"location":"userguide/configuration/BAMBOO_CONFIGURATION/#bamboo-agents-configuration","text":"","title":"Bamboo Agents configuration"},{"location":"userguide/configuration/BAMBOO_CONFIGURATION/#agent-helm-chart-version","text":"bamboo_helm_chart_version sets the Helm chart version of Bamboo Agent instance. bamboo_agent_helm_chart_version = \"1.2.0\"","title":"Agent Helm chart version"},{"location":"userguide/configuration/BAMBOO_CONFIGURATION/#agent-version-tag","text":"Bamboo Agent will be installed with the default version defined in its Helm chart . If you want to install a specific version of the Agent, you can set the bamboo_agent_version_tag to the desired version. For more information, see Bamboo Agent Version Tags . bamboo_agent_version_tag = \"<BAMBOO_AGENT_VERSION_TAG>\"","title":"Agent version tag"},{"location":"userguide/configuration/BAMBOO_CONFIGURATION/#agent-instance-resource-configuration","text":"The following variables set number of CPU and amount of memory of Bamboo Agent instances. (Used default values as example.) bamboo_agent_cpu = \"0.25\" bamboo_agent_mem = \"256m\"","title":"Agent instance resource configuration"},{"location":"userguide/configuration/BAMBOO_CONFIGURATION/#number-of-agents","text":"number_of_bamboo_agents sets the number of remote agents to be launched. To disable agents, set this value to 0 . number_of_bamboo_agents = 5 The number of agents is limited to the number of allowed agents in your license. Any agents beyond the allowed number won't be able to join the cluster. A valid license is required to install bamboo agents Bamboo needs a valid license to install remote agents. Disable agents if you don't provide a license at installation time.","title":"Number of agents"},{"location":"userguide/configuration/BAMBOO_CONFIGURATION/#rds-configuration","text":"","title":"RDS Configuration"},{"location":"userguide/configuration/BAMBOO_CONFIGURATION/#database-engine-version","text":"bamboo_db_major_engine_version sets the PostgeSQL engine version that will be used. bamboo_db_major_engine_version = \"13\" Supported DB versions Be sure to use a DB engine version that is supported by Bamboo","title":"Database engine version"},{"location":"userguide/configuration/BAMBOO_CONFIGURATION/#database-instance-class","text":"bamboo_db_instance_class sets the DB instance type that allocates the computational, network, and memory capacity required by the planned workload of the DB instance. For more information about available instance classes, see DB instance classes \u2014 Amazon Relational Database Service . bamboo_db_instance_class = \"<INSTANCE_CLASS>\" # e.g. \"db.t3.micro\"","title":"Database Instance Class"},{"location":"userguide/configuration/BAMBOO_CONFIGURATION/#database-allocated-storage","text":"bamboo_db_allocated_storage sets the allocated storage for the database instance in GiB. bamboo_db_allocated_storage = 100 The allowed value range of allocated storage may vary based on instance class You may want to adjust these values according to your needs. For more information, see Amazon RDS DB instance storage \u2014 Amazon Relational Database Service .","title":"Database Allocated Storage"},{"location":"userguide/configuration/BAMBOO_CONFIGURATION/#database-iops","text":"bamboo_db_iops sets the requested number of I/O operations per second that the DB instance can support. bamboo_db_iops = 1000 The allowed value range of IOPS may vary based on instance class You may want to adjust these values according to your needs. For more information, see Amazon RDS DB instance storage \u2014 Amazon Relational Database Service .","title":"Database IOPS"},{"location":"userguide/configuration/BAMBOO_CONFIGURATION/#database-name","text":"bamboo_db_name defines the name of database to be used for the Bamboo in RDS instance. bamboo_db_name = \"bamboo\"","title":"Database name"},{"location":"userguide/configuration/BAMBOO_CONFIGURATION/#restoring-from-backup","text":"To restore data from an existing Bamboo backup , you can set the dataset_url variable to a publicly accessible URL where the dataset can be downloaded. dataset_url = \"https://bamboo-test-datasets.s3.amazonaws.com/dcapt-bamboo-no-agents.zip\" This dataset is downloaded to the shared home and then imported by the Bamboo instance. To log in to the instance, you will need to use any credentials from the dataset. Provisioning time Restoring from the dataset will increase the time it takes to create the environment.","title":"Restoring from Backup"},{"location":"userguide/configuration/BITBUCKET_CONFIGURATION/","text":"Bitbucket configuration \u00b6 Application configuration \u00b6 Helm chart version \u00b6 bitbucket_helm_chart_version sets the Helm chart version of Bitbucket instance. Bitbucket_helm_chart_version = \"1.2.0\" Bitbucket version tag \u00b6 Bitbucket will be installed with the default version defined in its Helm chart . If you want to install a specific version of Bitbucket, you can set the bitbucket_version_tag to the version you want to install. For more information, see Bitbucket Version Tags . bitbucket_version_tag = \"<BITBUCKET_VERSION_TAG>\" Number of Bitbucket application nodes \u00b6 bitbucket_replica_count defines the desired number of application nodes. If you desire to install more than one application node, you must include System Admin Credentials and License properties listed below in the first installation. This ensures Bitbucket is using fully automated setup. bitbucket_replica_count = 1 Cluster size Cluster Autoscaler installed in the cluster will monitor the amount of required resources and adjust the cluster size to accommodate the requested cpu and memory. Installation timeout \u00b6 bitbucket_installation_timeout defines the timeout (in minutes) for product helm chart installation . Different variables can influence how long it takes the application from installation to ready state. These can be dataset restoration, resource requirements, number of replicas and others. bitbucket_installation_timeout = 10 License \u00b6 bitbucket_license takes the license key of Bitbucket product. Make sure that there is no new lines or spaces in license key. bitbucket_license = \"<LICENSE_KEY>\" Sensitive data bitbucket_license is marked as sensitive, storing in a plain-text config.tfvars file is not recommended. Please refer to Sensitive Data section. System Admin Credentials \u00b6 Four values are optional to configure Bitbucket system admin credentials. If those values are not provided, then Bitbucket will start in setup page to complete the system admin configuration. bitbucket_admin_username = \"<USERNAME>\" bitbucket_admin_password = \"<PASSWORD>\" bitbucket_admin_display_name = \"<DISPLAY_NAME>\" bitbucket_admin_email_address = \"<EMAIL_ADDRESS>\" Sensitive data bitbucket_admin_password is marked as sensitive, storing in a plain-text config.tfvars file is not recommended. Please refer to Sensitive Data section. Display Name \u00b6 Set the display name of the Bitbucket instance. Note that this value is only used during installation and changing the value during an upgrade has no effect. bitbucket_display_name = \"<DISPLAY_NAME>\" Instance resource configuration \u00b6 The following variables set number of CPU, amount of memory, maximum heap size and minimum heap size of Bitbucket instance. (Used default values as example.) bitbucket_cpu = \"1\" bitbucket_mem = \"1Gi\" bitbucket_min_heap = \"256m\" bitbucket_max_heap = \"512m\" RDS Configuration \u00b6 Database engine version \u00b6 bitbucket_db_major_engine_version sets the PostgeSQL engine version that will be used. bitbucket_db_major_engine_version = \"14\" Supported DB versions Be sure to use a DB engine version that is supported by Bitbucket Restore from snapshot This value is ignored if RDS snaphost is provided with bitbucket_db_snapshot_id . Database Instance Class \u00b6 bitbucket_db_instance_class sets the DB instance type that allocates the computational, network, and memory capacity required by the planned workload of the DB instance. For more information about available instance classes, see DB instance classes \u2014 Amazon Relational Database Service . bitbucket_db_instance_class = \"<INSTANCE_CLASS>\" # e.g. \"db.t3.micro\" Database Allocated Storage \u00b6 bitbucket_db_allocated_storage sets the allocated storage for the database instance in GiB. bitbucket_db_allocated_storage = 100 The allowed value range of allocated storage may vary based on instance class You may want to adjust these values according to your needs. For more information, see Amazon RDS DB instance storage \u2014 Amazon Relational Database Service . Database IOPS \u00b6 bitbucket_db_iops sets the requested number of I/O operations per second that the DB instance can support. bitbucket_db_iops = 1000 The allowed value range of IOPS may vary based on instance class You may want to adjust these values according to your needs. For more information, see Amazon RDS DB instance storage \u2014 Amazon Relational Database Service . Database name \u00b6 bitbucket_db_name defines the name of database to be used for the Bitbucket in RDS instance. If you restore the database, you need to provide the db name from the snapshot. If the snapshot does not have default db name, then set this variable to null . bitbucket_db_name = \"bitbucket\" NFS and OpenSearch Configuration \u00b6 NFS resource configuration \u00b6 The following variables set the initial cpu/memory request sizes including their limits for the NFS instance. (Default values used as example.) # Bitbucket NFS instance resource configuration bitbucket_nfs_requests_cpu = \"1\" bitbucket_nfs_requests_memory = \"1Gi\" bitbucket_nfs_limits_cpu = \"2\" bitbucket_nfs_limits_memory = \"2Gi\" OpenSearch Configuration \u00b6 The following variables set the request for number of CPU, amount of memory, amount of storage, and the number of instances in elasticsearch cluster. # Elasticsearch resource configuration for Bitbucket bitbucket_opensearch_requests_cpu = \"2\" bitbucket_opensearch_requests_memory = \"2Gi\" bitbucket_opensearch_limits_cpu = \"4\" bitbucket_opensearch_limits_memory = \"4Gi\" bitbucket_opensearch_storage = \"20\" bitbucket_opensearch_replicas = \"3\" bitbucket_opensearch_java_opts = \"-Xmx=1024\" #Configure access to external OpenSearch (created outside Terraform modules) bitbucket_opensearch_endpoint = \"https://myopensearch.com\" bitbucket_opensearch_secret_name = \"os-creds\" bitbucket_opensearch_secret_username_key = \"user\" bitbucket_opensearch_secret_password_key = \"pass\" Dataset restore configuration \u00b6 To restore the dataset into the newly created instance, uncomment the following lines and provide all necessary parameters. Database Snapshot Identifier \u00b6 bitbucket_db_snapshot_id sets the identifier for the DB snapshot to restore from. If you do not specify a value, no AWS RDS snapshot is used. bitbucket_db_snapshot_id = \"<SNAPSHOT_IDENTIFIER>\" # e.g. \"my-snapshot\" The AWS RDS snapshot must be in the same region and account as the RDS instance. You also need to provide the master user credentials ( bitbucket_db_master_username and bitbucket_db_master_password ) that match the snapshot. Optimise the restore performance. To obtain the best performance, configure Bitbucket RDS that match the snapshot including bitbucket_db_instance_class and bitbucket_db_allocated_storage . Database Master Username \u00b6 'bitbucket_db_master_username' sets the username for the RDS master user. If you do not specify a value, username is \"postgres\". bitbucket_db_master_username = \"<DB_MASTER_USERNAME>\" # e.g. \"postgres\" Database Master Password \u00b6 'bitbucket_db_master_password' sets the password for the RDS master user. If you do not specify a value, a random password will be generated. bitbucket_db_master_password = \"<DB_MASTER_PASSWORD>\" # default value is null Shared home snapshot id \u00b6 To restore a shared home dataset, you can provide an EBS snapshot ID that contains the content of the shared home volume. This volume will then be mounted to the NFS server and used when the product is started. bitbucket_shared_home_snapshot_id sets the id of the shared home EBS snapshot to use. This will spin up an EBS volume that will be mounted to the NFS server and used when the product is started. Make sure the snapshot is available in the region you are deploying to and follows all product requirements. bitbucket_shared_home_snapshot_id = \"<SHARED_HOME_EBS_SNAPSHOT_IDENTIFIER>\" Snapshot and your environment must be in same region","title":"Bitbucket configuration"},{"location":"userguide/configuration/BITBUCKET_CONFIGURATION/#bitbucket-configuration","text":"","title":"Bitbucket configuration"},{"location":"userguide/configuration/BITBUCKET_CONFIGURATION/#application-configuration","text":"","title":"Application configuration"},{"location":"userguide/configuration/BITBUCKET_CONFIGURATION/#helm-chart-version","text":"bitbucket_helm_chart_version sets the Helm chart version of Bitbucket instance. Bitbucket_helm_chart_version = \"1.2.0\"","title":"Helm chart version"},{"location":"userguide/configuration/BITBUCKET_CONFIGURATION/#bitbucket-version-tag","text":"Bitbucket will be installed with the default version defined in its Helm chart . If you want to install a specific version of Bitbucket, you can set the bitbucket_version_tag to the version you want to install. For more information, see Bitbucket Version Tags . bitbucket_version_tag = \"<BITBUCKET_VERSION_TAG>\"","title":"Bitbucket version tag"},{"location":"userguide/configuration/BITBUCKET_CONFIGURATION/#number-of-bitbucket-application-nodes","text":"bitbucket_replica_count defines the desired number of application nodes. If you desire to install more than one application node, you must include System Admin Credentials and License properties listed below in the first installation. This ensures Bitbucket is using fully automated setup. bitbucket_replica_count = 1 Cluster size Cluster Autoscaler installed in the cluster will monitor the amount of required resources and adjust the cluster size to accommodate the requested cpu and memory.","title":"Number of Bitbucket application nodes"},{"location":"userguide/configuration/BITBUCKET_CONFIGURATION/#installation-timeout","text":"bitbucket_installation_timeout defines the timeout (in minutes) for product helm chart installation . Different variables can influence how long it takes the application from installation to ready state. These can be dataset restoration, resource requirements, number of replicas and others. bitbucket_installation_timeout = 10","title":"Installation timeout"},{"location":"userguide/configuration/BITBUCKET_CONFIGURATION/#license","text":"bitbucket_license takes the license key of Bitbucket product. Make sure that there is no new lines or spaces in license key. bitbucket_license = \"<LICENSE_KEY>\" Sensitive data bitbucket_license is marked as sensitive, storing in a plain-text config.tfvars file is not recommended. Please refer to Sensitive Data section.","title":"License"},{"location":"userguide/configuration/BITBUCKET_CONFIGURATION/#system-admin-credentials","text":"Four values are optional to configure Bitbucket system admin credentials. If those values are not provided, then Bitbucket will start in setup page to complete the system admin configuration. bitbucket_admin_username = \"<USERNAME>\" bitbucket_admin_password = \"<PASSWORD>\" bitbucket_admin_display_name = \"<DISPLAY_NAME>\" bitbucket_admin_email_address = \"<EMAIL_ADDRESS>\" Sensitive data bitbucket_admin_password is marked as sensitive, storing in a plain-text config.tfvars file is not recommended. Please refer to Sensitive Data section.","title":"System Admin Credentials"},{"location":"userguide/configuration/BITBUCKET_CONFIGURATION/#display-name","text":"Set the display name of the Bitbucket instance. Note that this value is only used during installation and changing the value during an upgrade has no effect. bitbucket_display_name = \"<DISPLAY_NAME>\"","title":"Display Name"},{"location":"userguide/configuration/BITBUCKET_CONFIGURATION/#instance-resource-configuration","text":"The following variables set number of CPU, amount of memory, maximum heap size and minimum heap size of Bitbucket instance. (Used default values as example.) bitbucket_cpu = \"1\" bitbucket_mem = \"1Gi\" bitbucket_min_heap = \"256m\" bitbucket_max_heap = \"512m\"","title":"Instance resource configuration"},{"location":"userguide/configuration/BITBUCKET_CONFIGURATION/#rds-configuration","text":"","title":"RDS Configuration"},{"location":"userguide/configuration/BITBUCKET_CONFIGURATION/#database-engine-version","text":"bitbucket_db_major_engine_version sets the PostgeSQL engine version that will be used. bitbucket_db_major_engine_version = \"14\" Supported DB versions Be sure to use a DB engine version that is supported by Bitbucket Restore from snapshot This value is ignored if RDS snaphost is provided with bitbucket_db_snapshot_id .","title":"Database engine version"},{"location":"userguide/configuration/BITBUCKET_CONFIGURATION/#database-instance-class","text":"bitbucket_db_instance_class sets the DB instance type that allocates the computational, network, and memory capacity required by the planned workload of the DB instance. For more information about available instance classes, see DB instance classes \u2014 Amazon Relational Database Service . bitbucket_db_instance_class = \"<INSTANCE_CLASS>\" # e.g. \"db.t3.micro\"","title":"Database Instance Class"},{"location":"userguide/configuration/BITBUCKET_CONFIGURATION/#database-allocated-storage","text":"bitbucket_db_allocated_storage sets the allocated storage for the database instance in GiB. bitbucket_db_allocated_storage = 100 The allowed value range of allocated storage may vary based on instance class You may want to adjust these values according to your needs. For more information, see Amazon RDS DB instance storage \u2014 Amazon Relational Database Service .","title":"Database Allocated Storage"},{"location":"userguide/configuration/BITBUCKET_CONFIGURATION/#database-iops","text":"bitbucket_db_iops sets the requested number of I/O operations per second that the DB instance can support. bitbucket_db_iops = 1000 The allowed value range of IOPS may vary based on instance class You may want to adjust these values according to your needs. For more information, see Amazon RDS DB instance storage \u2014 Amazon Relational Database Service .","title":"Database IOPS"},{"location":"userguide/configuration/BITBUCKET_CONFIGURATION/#database-name","text":"bitbucket_db_name defines the name of database to be used for the Bitbucket in RDS instance. If you restore the database, you need to provide the db name from the snapshot. If the snapshot does not have default db name, then set this variable to null . bitbucket_db_name = \"bitbucket\"","title":"Database name"},{"location":"userguide/configuration/BITBUCKET_CONFIGURATION/#nfs-and-opensearch-configuration","text":"","title":"NFS and OpenSearch Configuration"},{"location":"userguide/configuration/BITBUCKET_CONFIGURATION/#nfs-resource-configuration","text":"The following variables set the initial cpu/memory request sizes including their limits for the NFS instance. (Default values used as example.) # Bitbucket NFS instance resource configuration bitbucket_nfs_requests_cpu = \"1\" bitbucket_nfs_requests_memory = \"1Gi\" bitbucket_nfs_limits_cpu = \"2\" bitbucket_nfs_limits_memory = \"2Gi\"","title":"NFS resource configuration"},{"location":"userguide/configuration/BITBUCKET_CONFIGURATION/#opensearch-configuration","text":"The following variables set the request for number of CPU, amount of memory, amount of storage, and the number of instances in elasticsearch cluster. # Elasticsearch resource configuration for Bitbucket bitbucket_opensearch_requests_cpu = \"2\" bitbucket_opensearch_requests_memory = \"2Gi\" bitbucket_opensearch_limits_cpu = \"4\" bitbucket_opensearch_limits_memory = \"4Gi\" bitbucket_opensearch_storage = \"20\" bitbucket_opensearch_replicas = \"3\" bitbucket_opensearch_java_opts = \"-Xmx=1024\" #Configure access to external OpenSearch (created outside Terraform modules) bitbucket_opensearch_endpoint = \"https://myopensearch.com\" bitbucket_opensearch_secret_name = \"os-creds\" bitbucket_opensearch_secret_username_key = \"user\" bitbucket_opensearch_secret_password_key = \"pass\"","title":"OpenSearch Configuration"},{"location":"userguide/configuration/BITBUCKET_CONFIGURATION/#dataset-restore-configuration","text":"To restore the dataset into the newly created instance, uncomment the following lines and provide all necessary parameters.","title":"Dataset restore configuration"},{"location":"userguide/configuration/BITBUCKET_CONFIGURATION/#database-snapshot-identifier","text":"bitbucket_db_snapshot_id sets the identifier for the DB snapshot to restore from. If you do not specify a value, no AWS RDS snapshot is used. bitbucket_db_snapshot_id = \"<SNAPSHOT_IDENTIFIER>\" # e.g. \"my-snapshot\" The AWS RDS snapshot must be in the same region and account as the RDS instance. You also need to provide the master user credentials ( bitbucket_db_master_username and bitbucket_db_master_password ) that match the snapshot. Optimise the restore performance. To obtain the best performance, configure Bitbucket RDS that match the snapshot including bitbucket_db_instance_class and bitbucket_db_allocated_storage .","title":"Database Snapshot Identifier"},{"location":"userguide/configuration/BITBUCKET_CONFIGURATION/#database-master-username","text":"'bitbucket_db_master_username' sets the username for the RDS master user. If you do not specify a value, username is \"postgres\". bitbucket_db_master_username = \"<DB_MASTER_USERNAME>\" # e.g. \"postgres\"","title":"Database Master Username"},{"location":"userguide/configuration/BITBUCKET_CONFIGURATION/#database-master-password","text":"'bitbucket_db_master_password' sets the password for the RDS master user. If you do not specify a value, a random password will be generated. bitbucket_db_master_password = \"<DB_MASTER_PASSWORD>\" # default value is null","title":"Database Master Password"},{"location":"userguide/configuration/BITBUCKET_CONFIGURATION/#shared-home-snapshot-id","text":"To restore a shared home dataset, you can provide an EBS snapshot ID that contains the content of the shared home volume. This volume will then be mounted to the NFS server and used when the product is started. bitbucket_shared_home_snapshot_id sets the id of the shared home EBS snapshot to use. This will spin up an EBS volume that will be mounted to the NFS server and used when the product is started. Make sure the snapshot is available in the region you are deploying to and follows all product requirements. bitbucket_shared_home_snapshot_id = \"<SHARED_HOME_EBS_SNAPSHOT_IDENTIFIER>\" Snapshot and your environment must be in same region","title":"Shared home snapshot id"},{"location":"userguide/configuration/CONFIGURATION/","text":"Common configuration \u00b6 In order to provision the infrastructure and install an Atlassian Data Center product, you need to create a valid Terraform configuration . All configuration data should go to a Terraform configuration file. The content of the configuration file is divided into two groups: Common configuration Product specific configuration Configuration file format. The configuration file is an ASCII text file with the .tfvars extension. The config file must contain all mandatory configuration items with valid values. If any optional items are missing, the default values will be applied. The mandatory configuration items are those you should define once before the first installation. Mandatory values cannot be changed during the entire environment lifecycle. The optional configuration items are not required for installation by default. Optional values may change at any point in the environment lifecycle. Terraform will retain the latest state of the environment and keep track of any configuration changes made later. The following is an example of a valid configuration file: # Mandatory items environment_name = \"my-bamboo-env\" region = \"us-east-2\" # Optional items resource_tags = { Terraform = \"true\" , Organization = \"atlassian\" , product = \"bamboo\" , } instance_types = [ \"m5.xlarge\" ] desired_capacity = 2 domain = \"mydomain.com\" Common Configuration \u00b6 Environmental properties common to all deployments. Environment Name \u00b6 environment_name provides your environment a unique name within a single cloud provider account. This value cannot be altered after the configuration has been applied. The value will be used to form the name of some resources including VPC and Kubernetes cluster . environment_name = \"<your-environment-name>\" # e.g. \"my-terraform-env\" Format Environment names should start with a letter and can contain letters, numbers, and dashes ( - ). The maximum value length is 24 characters. EKS K8S API version \u00b6 eks_version is the supported EKS K8S API version. It must be a valid EKS version . Latest EKS version It is recommended to use the default value, however it is possible to override it to try a different (the latest) EKS version for experimental purposes. Region \u00b6 region defines the cloud provider region that the environment will be deployed to. region = \"<REGION>\" # e.g. \"ap-northeast-2\" Format The value must be a valid AWS region . Products \u00b6 The products list can be configured with one or multiple products. This will result in these products being deployed to the same K8s cluster. For example, if a Jira and Confluence deployment is required this property can be configured as follows: products = [ \"jira\", \"confluence\" ] Available values jira , confluence , bitbucket , bamboo Whitelist IP blocks \u00b6 whitelist_cidr defines a set of CIDRs that are allowed to run the applications. By default, the deployed applications are publicly accessible. You can restrict this access by changing the default value to your desired CIDR blocks that are allowed to run the applications. whitelist_cidr = [ \"199.0.0.0/8\", \"119.81.0.0/16\" ] Domain \u00b6 We recommend using a domain name to access the application via HTTPS . You will be required to secure a domain name and supply the configuration to the config file. When the domain is provided, Terraform will create a Route53 hosted zone based on the environment name. domain = \"<DOMAIN_NAME>\" # e.g. \"mydomain.com\" A fully qualified domain name uses the following format: <product>.<environment-name>.<domain-name> . For example bamboo.staging.mydomain.com . Ingress controller If a domain name is defined, Terraform will create a nginx-ingress controller in the EKS cluster that will provide access to the application via the domain name. Terraform will also create an ACM certificate to provide secure connections over HTTPS. Provision the infrastructure without a domain When commented out the product will be exposed via an unsecured ( HTTP only) DNS endpoint automatically provisioned as part of the AWS ELB load balancer, for example: http://<load-balancer-id>.<region>.elb.amazonaws.com . This DNS Name will be printed out as part of the outputs after the infrastructure has been provisioned. Resource tags \u00b6 resource_tags are custom metadata for all resources in the environment. You can provide multiple tags as a list. Tag propagation Tag names must be unique, and tags will be propogated to all provisioned resources. resource_tags = { <tag-name- 0 > = \"<tag-value>\" , <tag-name- 1 > = \"<tag-value>\" , ... <tag-name-n> = \"<tag-value>\" , } Using Terraform CLI to apply tags is not recommended and may lead to missing tags in some resources. To apply tags to all resources, follow the installation guide . EKS instance type and storage size \u00b6 instance_types defines the instance type for the EKS cluster node group. instance_types = [ \"m5.2xlarge\" ] The instance type must be a valid AWS instance type . instance_disk_size defines the size of default storage attached to an instance. instance_disk_size = 50 Instance type and disk size selection Both properties cannot be changed once the infrastructure has been provisioned. EKS Node Launch Template \u00b6 When Terraform creates a node group for the EKS cluster, the default launch template is created behind the scenes. However, if you need to install any additional tooling/software in the worker node EC2 instances, you may provide your own template in data-center-terraform/modules/AWS/eks/nodegroup_launch_template/templates . This needs to be a file with .tlp extension. See: Amazon EC2 user data . Here's an example: MIME-Version: 1.0 Content-Type: multipart/mixed; boundary=\"==MYBOUNDARY==\" --==MYBOUNDARY== Content-Type: text/x-shellscript; charset=\"us-ascii\" #!/bin/bash echo \"Running custom user data script\" --==MYBOUNDARY==-- Templates in data-center-terraform/modules/AWS/eks/nodegroup_launch_template/templates will be merged with the default launch template. If you need to use environment variables in your custom scripts, make sure you escape them with an additional dollar sign, otherwise templatefile function will complain about a missing env var: foo=\"bar\" echo $${foo} Environment variables passed to templatefile function are not configurable (defined in data-center-terraform/modules/AWS/eks/nodegroup_launch_template/locals.tf ), thus it makes sense to generate template outside terraform and pull it before installing/upgrading. Cluster size \u00b6 EKS cluster creates an Autoscaling Group (ASG) that has defined minimum and maximum capacity. You are able to set these values in the config file: Minimum values are 1 and maximum is 20 . min_cluster_capacity = 1 # between 1 and 20 max_cluster_capacity = 5 # between 1 and 20 Cluster size and cost In the installation process, cluster-autoscaler is installed in the Kubernetes cluster. The number of nodes will be automatically adjusted depending on the workload resource requirements. Additional IAM roles \u00b6 When the EKS cluster is created, only the entity that created the cluster can access and list resources inside the cluster. To enable access for additional roles, you can add them to the config file: eks_additional_roles = { user = { kubernetes_group = [] principal_arn = \"arn:aws:iam::121212121212:role/test-policy-role\" policy_associations = { admin = { policy_arn = \"arn:aws:eks::aws:cluster-access-policy/AmazonEKSClusterAdminPolicy\" access_scope = { namespaces = [] type = \"cluster\" } } } } } Access Entries in AWS EKS For additional information regarding adding access entries in EKS cluster, follow the official AWS documentation . Logging S3 bucket name \u00b6 If you wish to log activities of terraform backend create S3 bucket and provide the name of the S3 bucket as follows. This will allow the terraform script to link your terraform backend to logging bucket. logging_bucket = <LOGGING_S 3 _BUCKET_NAME> # default is null S3 Logging bucket Creation Providing logging_bucket will not guarantee the creation of the S3 Bucket. You will need to create one as part of the prerequisites. Monitoring \u00b6 If you want to deploy a monitoring stack to the cluster, use the following variable in config.tfvars file: monitoring_enabled = true When enabled, Terraform will deploy kube-prometheus-stack Helm chart with Prometheus, AlertManager, Node Exporter and Grafana. By default, Grafana service isn't exposed, and you can login to Grafana at http://localhost:3000 after running: kubectl port-forward $grafana-pod 3000:3000 -n kube-monitoring If you want to expose Grafana service as LoadBalancer , set monitoring_grafana_expose_lb to true : monitoring_grafana_expose_lb = true Run the following command to get Grafana service hostname: kubectl get svc -n kube-monitoring Out of the box Grafana is shipped with default Kubernetes dashboards which you can use to monitor pods health. You can also create own custom configmaps labeled grafana_dashboard=dc_monitoring , and Grafana sidecar will automatically import them. By default, both Prometheus and Grafana claim 10Gi of persistent storage. You can override the default values by setting: prometheus_pvc_disk_size = \"50Gi\" grafana_pvc_disk_size = \"20Gi\" Volume Expansion Out of the box EKS cluster is created with gp2 storage class which does not allow volume expansion, i.e. if you expect a high volume of metrics or metrics with high cardinality it is recommended to override the default Prometheus 10Gi PVC storage request when creating enabling monitoring for the first time. AWS documentation . Snapshot Configuration \u00b6 It is possible to restore DC products from snapshots. Each DC product requires a valid public RDS and EBS (shared-home) snapshot defined by the following variables: <product>_db_snapshot_id <product>_shared_home_snapshot_id Note that COnfluence and Crowd also require a build number: confluence_db_snapshot_build_number = \"8017\" crowd_db_snapshot_build_number = \"5023\" Snapshots must be public and exist in the target region. Snapshots JSON File It is also possible to use a special snapshots JSON file with pre-defined snapshot ID and build numbers for all products for both small and large dataset sizes. You can find example JSON in test/dcapt-snapshots.json . To use snapshots JSON rather than dedicated environment variables, set in config.tfvars : snapshots_json_file_path = \"test/dcapt-snapshots.json\" If snapshots_json_file_path snapshot variables defined in config.tfvars are ignored. Only use snapshots JSON suggested by DCAPT team . Product specific configuration \u00b6 Bamboo Confluence Jira Bamboo specific configuration Confluence specific configuration Jira specific configuration Sensitive Data \u00b6 Sensitive input data will eventually be stored as secrets within Kubernetes cluster . We use config.tfvars file to pass configuration values to Terraform stack. The file itself is plain-text on local machine, and will not be stored in remote backend where all the Terraform state files will be stored encrypted. More info regarding sensitive data in Terraform state can be found here . To avoid storing sensitive data in a plain-text file like config.tfvars , we recommend storing them in environment variables prefixed with TF_VAR_ . Take bamboo_admin_password for example, for Linux-like sytems, run the following command to write bamboo admin password to environment variable: export TF_VAR_bamboo_admin_password = <password> If storing this data as plain-text is not a particular concern for the environment to be deployed, you can also choose to supply the values in config.tfvars file. Uncomment the corresponding line and configure the value there.","title":"Common configuration"},{"location":"userguide/configuration/CONFIGURATION/#common-configuration","text":"In order to provision the infrastructure and install an Atlassian Data Center product, you need to create a valid Terraform configuration . All configuration data should go to a Terraform configuration file. The content of the configuration file is divided into two groups: Common configuration Product specific configuration Configuration file format. The configuration file is an ASCII text file with the .tfvars extension. The config file must contain all mandatory configuration items with valid values. If any optional items are missing, the default values will be applied. The mandatory configuration items are those you should define once before the first installation. Mandatory values cannot be changed during the entire environment lifecycle. The optional configuration items are not required for installation by default. Optional values may change at any point in the environment lifecycle. Terraform will retain the latest state of the environment and keep track of any configuration changes made later. The following is an example of a valid configuration file: # Mandatory items environment_name = \"my-bamboo-env\" region = \"us-east-2\" # Optional items resource_tags = { Terraform = \"true\" , Organization = \"atlassian\" , product = \"bamboo\" , } instance_types = [ \"m5.xlarge\" ] desired_capacity = 2 domain = \"mydomain.com\"","title":"Common configuration"},{"location":"userguide/configuration/CONFIGURATION/#common-configuration_1","text":"Environmental properties common to all deployments.","title":"Common Configuration"},{"location":"userguide/configuration/CONFIGURATION/#environment-name","text":"environment_name provides your environment a unique name within a single cloud provider account. This value cannot be altered after the configuration has been applied. The value will be used to form the name of some resources including VPC and Kubernetes cluster . environment_name = \"<your-environment-name>\" # e.g. \"my-terraform-env\" Format Environment names should start with a letter and can contain letters, numbers, and dashes ( - ). The maximum value length is 24 characters.","title":"Environment Name"},{"location":"userguide/configuration/CONFIGURATION/#eks-k8s-api-version","text":"eks_version is the supported EKS K8S API version. It must be a valid EKS version . Latest EKS version It is recommended to use the default value, however it is possible to override it to try a different (the latest) EKS version for experimental purposes.","title":"EKS K8S API version"},{"location":"userguide/configuration/CONFIGURATION/#region","text":"region defines the cloud provider region that the environment will be deployed to. region = \"<REGION>\" # e.g. \"ap-northeast-2\" Format The value must be a valid AWS region .","title":"Region"},{"location":"userguide/configuration/CONFIGURATION/#products","text":"The products list can be configured with one or multiple products. This will result in these products being deployed to the same K8s cluster. For example, if a Jira and Confluence deployment is required this property can be configured as follows: products = [ \"jira\", \"confluence\" ] Available values jira , confluence , bitbucket , bamboo","title":"Products"},{"location":"userguide/configuration/CONFIGURATION/#whitelist-ip-blocks","text":"whitelist_cidr defines a set of CIDRs that are allowed to run the applications. By default, the deployed applications are publicly accessible. You can restrict this access by changing the default value to your desired CIDR blocks that are allowed to run the applications. whitelist_cidr = [ \"199.0.0.0/8\", \"119.81.0.0/16\" ]","title":"Whitelist IP blocks"},{"location":"userguide/configuration/CONFIGURATION/#domain","text":"We recommend using a domain name to access the application via HTTPS . You will be required to secure a domain name and supply the configuration to the config file. When the domain is provided, Terraform will create a Route53 hosted zone based on the environment name. domain = \"<DOMAIN_NAME>\" # e.g. \"mydomain.com\" A fully qualified domain name uses the following format: <product>.<environment-name>.<domain-name> . For example bamboo.staging.mydomain.com . Ingress controller If a domain name is defined, Terraform will create a nginx-ingress controller in the EKS cluster that will provide access to the application via the domain name. Terraform will also create an ACM certificate to provide secure connections over HTTPS. Provision the infrastructure without a domain When commented out the product will be exposed via an unsecured ( HTTP only) DNS endpoint automatically provisioned as part of the AWS ELB load balancer, for example: http://<load-balancer-id>.<region>.elb.amazonaws.com . This DNS Name will be printed out as part of the outputs after the infrastructure has been provisioned.","title":"Domain"},{"location":"userguide/configuration/CONFIGURATION/#resource-tags","text":"resource_tags are custom metadata for all resources in the environment. You can provide multiple tags as a list. Tag propagation Tag names must be unique, and tags will be propogated to all provisioned resources. resource_tags = { <tag-name- 0 > = \"<tag-value>\" , <tag-name- 1 > = \"<tag-value>\" , ... <tag-name-n> = \"<tag-value>\" , } Using Terraform CLI to apply tags is not recommended and may lead to missing tags in some resources. To apply tags to all resources, follow the installation guide .","title":"Resource tags"},{"location":"userguide/configuration/CONFIGURATION/#eks-instance-type-and-storage-size","text":"instance_types defines the instance type for the EKS cluster node group. instance_types = [ \"m5.2xlarge\" ] The instance type must be a valid AWS instance type . instance_disk_size defines the size of default storage attached to an instance. instance_disk_size = 50 Instance type and disk size selection Both properties cannot be changed once the infrastructure has been provisioned.","title":"EKS instance type and storage size"},{"location":"userguide/configuration/CONFIGURATION/#eks-node-launch-template","text":"When Terraform creates a node group for the EKS cluster, the default launch template is created behind the scenes. However, if you need to install any additional tooling/software in the worker node EC2 instances, you may provide your own template in data-center-terraform/modules/AWS/eks/nodegroup_launch_template/templates . This needs to be a file with .tlp extension. See: Amazon EC2 user data . Here's an example: MIME-Version: 1.0 Content-Type: multipart/mixed; boundary=\"==MYBOUNDARY==\" --==MYBOUNDARY== Content-Type: text/x-shellscript; charset=\"us-ascii\" #!/bin/bash echo \"Running custom user data script\" --==MYBOUNDARY==-- Templates in data-center-terraform/modules/AWS/eks/nodegroup_launch_template/templates will be merged with the default launch template. If you need to use environment variables in your custom scripts, make sure you escape them with an additional dollar sign, otherwise templatefile function will complain about a missing env var: foo=\"bar\" echo $${foo} Environment variables passed to templatefile function are not configurable (defined in data-center-terraform/modules/AWS/eks/nodegroup_launch_template/locals.tf ), thus it makes sense to generate template outside terraform and pull it before installing/upgrading.","title":"EKS Node Launch Template"},{"location":"userguide/configuration/CONFIGURATION/#cluster-size","text":"EKS cluster creates an Autoscaling Group (ASG) that has defined minimum and maximum capacity. You are able to set these values in the config file: Minimum values are 1 and maximum is 20 . min_cluster_capacity = 1 # between 1 and 20 max_cluster_capacity = 5 # between 1 and 20 Cluster size and cost In the installation process, cluster-autoscaler is installed in the Kubernetes cluster. The number of nodes will be automatically adjusted depending on the workload resource requirements.","title":"Cluster size"},{"location":"userguide/configuration/CONFIGURATION/#additional-iam-roles","text":"When the EKS cluster is created, only the entity that created the cluster can access and list resources inside the cluster. To enable access for additional roles, you can add them to the config file: eks_additional_roles = { user = { kubernetes_group = [] principal_arn = \"arn:aws:iam::121212121212:role/test-policy-role\" policy_associations = { admin = { policy_arn = \"arn:aws:eks::aws:cluster-access-policy/AmazonEKSClusterAdminPolicy\" access_scope = { namespaces = [] type = \"cluster\" } } } } } Access Entries in AWS EKS For additional information regarding adding access entries in EKS cluster, follow the official AWS documentation .","title":"Additional IAM roles"},{"location":"userguide/configuration/CONFIGURATION/#logging-s3-bucket-name","text":"If you wish to log activities of terraform backend create S3 bucket and provide the name of the S3 bucket as follows. This will allow the terraform script to link your terraform backend to logging bucket. logging_bucket = <LOGGING_S 3 _BUCKET_NAME> # default is null S3 Logging bucket Creation Providing logging_bucket will not guarantee the creation of the S3 Bucket. You will need to create one as part of the prerequisites.","title":"Logging S3 bucket name"},{"location":"userguide/configuration/CONFIGURATION/#monitoring","text":"If you want to deploy a monitoring stack to the cluster, use the following variable in config.tfvars file: monitoring_enabled = true When enabled, Terraform will deploy kube-prometheus-stack Helm chart with Prometheus, AlertManager, Node Exporter and Grafana. By default, Grafana service isn't exposed, and you can login to Grafana at http://localhost:3000 after running: kubectl port-forward $grafana-pod 3000:3000 -n kube-monitoring If you want to expose Grafana service as LoadBalancer , set monitoring_grafana_expose_lb to true : monitoring_grafana_expose_lb = true Run the following command to get Grafana service hostname: kubectl get svc -n kube-monitoring Out of the box Grafana is shipped with default Kubernetes dashboards which you can use to monitor pods health. You can also create own custom configmaps labeled grafana_dashboard=dc_monitoring , and Grafana sidecar will automatically import them. By default, both Prometheus and Grafana claim 10Gi of persistent storage. You can override the default values by setting: prometheus_pvc_disk_size = \"50Gi\" grafana_pvc_disk_size = \"20Gi\" Volume Expansion Out of the box EKS cluster is created with gp2 storage class which does not allow volume expansion, i.e. if you expect a high volume of metrics or metrics with high cardinality it is recommended to override the default Prometheus 10Gi PVC storage request when creating enabling monitoring for the first time. AWS documentation .","title":"Monitoring"},{"location":"userguide/configuration/CONFIGURATION/#snapshot-configuration","text":"It is possible to restore DC products from snapshots. Each DC product requires a valid public RDS and EBS (shared-home) snapshot defined by the following variables: <product>_db_snapshot_id <product>_shared_home_snapshot_id Note that COnfluence and Crowd also require a build number: confluence_db_snapshot_build_number = \"8017\" crowd_db_snapshot_build_number = \"5023\" Snapshots must be public and exist in the target region. Snapshots JSON File It is also possible to use a special snapshots JSON file with pre-defined snapshot ID and build numbers for all products for both small and large dataset sizes. You can find example JSON in test/dcapt-snapshots.json . To use snapshots JSON rather than dedicated environment variables, set in config.tfvars : snapshots_json_file_path = \"test/dcapt-snapshots.json\" If snapshots_json_file_path snapshot variables defined in config.tfvars are ignored. Only use snapshots JSON suggested by DCAPT team .","title":"Snapshot Configuration"},{"location":"userguide/configuration/CONFIGURATION/#product-specific-configuration","text":"Bamboo Confluence Jira Bamboo specific configuration Confluence specific configuration Jira specific configuration","title":"Product specific configuration"},{"location":"userguide/configuration/CONFIGURATION/#sensitive-data","text":"Sensitive input data will eventually be stored as secrets within Kubernetes cluster . We use config.tfvars file to pass configuration values to Terraform stack. The file itself is plain-text on local machine, and will not be stored in remote backend where all the Terraform state files will be stored encrypted. More info regarding sensitive data in Terraform state can be found here . To avoid storing sensitive data in a plain-text file like config.tfvars , we recommend storing them in environment variables prefixed with TF_VAR_ . Take bamboo_admin_password for example, for Linux-like sytems, run the following command to write bamboo admin password to environment variable: export TF_VAR_bamboo_admin_password = <password> If storing this data as plain-text is not a particular concern for the environment to be deployed, you can also choose to supply the values in config.tfvars file. Uncomment the corresponding line and configure the value there.","title":"Sensitive Data"},{"location":"userguide/configuration/CONFLUENCE_CONFIGURATION/","text":"Confluence configuration \u00b6 Application configuration \u00b6 Helm chart version \u00b6 confluence_helm_chart_version sets the Helm chart version of Confluence instance. confluence_helm_chart_version = \"1.2.0\" Confluence version tag \u00b6 Confluence will be installed with the default version defined in its Helm chart . If you want to install a specific version of Bamboo, you can set the confluence_version_tag to the version you want to install. For more information, see Confluence Version Tags . confluence_version_tag = \"<CONFLUENCE_VERSION_TAG>\" Number of Confluence application nodes \u00b6 The initial Confluence installation needs to be started with a single application node. After all the setup steps are finished, it is possible to update the confluence_replica_count with a number higher than 1 and run install.sh to update the application node count. # Number of Confluence application nodes # Note: For initial installation this value needs to be set to 1 and it can be changed only after Confluence is fully # installed and configured. confluence_replica_count = 1 Cluster size Cluster Autoscaler installed in the cluster will monitor the amount of required resources and adjust the cluster size to accomodate the requested cpu and memory. Installation timeout \u00b6 confluence_installation_timeout defines the timeout (in minutes) for product helm chart installation . Different variables can influence how long it takes the application from installation to ready state. These can be dataset restoration, resource requirements, number of replicas and others. confluence_installation_timeout = 10 License \u00b6 confluence_license takes the license key of Confluence product. Make sure that there is no new lines or spaces in license key. confluence_license = \"<LICENSE_KEY>\" Sensitive data confluence_license is marked as sensitive, storing in a plain-text config.tfvars file is not recommended. Please refer to Sensitive Data section. Confluence instance resource configuration \u00b6 The following variables set number of CPU, amount of memory, maximum heap size and minimum heap size of Confluence instance. (Used default values as example.) confluence_cpu = \"2\" confluence_mem = \"1Gi\" confluence_min_heap = \"256m\" confluence_max_heap = \"512m\" Synchrony instance resource configuration \u00b6 The following variables set number of CPU, amount of memory, maximum heap size, minimum heap size, and stack size of Synchrony instance. (Used default values as example.) synchrony_cpu = \"2\" synchrony_mem = \"2.5Gi\" synchrony_min_heap = \"1g\" synchrony_max_heap = \"2g\" synchrony_stack_size = \"2048k\" Collaborative editing \u00b6 confluence_collaborative_editing_enabled enables Collaborative editing . (default: true ) confluence_collaborative_editing_enabled = true RDS Configuration \u00b6 Database engine version \u00b6 confluence_db_major_engine_version sets the PostgeSQL engine version that will be used. confluence_db_major_engine_version = \"14\" Supported DB versions Be sure to use a DB engine version that is supported by Confluence Restore from snapshot This value is ignored if RDS snaphost is provided with confluence_db_snapshot_id . Database Instance Class \u00b6 confluence_db_instance_class sets the DB instance type that allocates the computational, network, and memory capacity required by the planned workload of the DB instance. For more information about available instance classes, see DB instance classes \u2014 Amazon Relational Database Service . confluence_db_instance_class = \"<INSTANCE_CLASS>\" # e.g. \"db.t3.micro\" Database Allocated Storage \u00b6 confluence_db_allocated_storage sets the allocated storage for the database instance in GiB. confluence_db_allocated_storage = 100 The allowed value range of allocated storage may vary based on instance class You may want to adjust these values according to your needs. For more information, see Amazon RDS DB instance storage \u2014 Amazon Relational Database Service . Database IOPS \u00b6 confluence_db_iops sets the requested number of I/O operations per second that the DB instance can support. confluence_db_iops = 1000 The allowed value range of IOPS may vary based on instance class You may want to adjust these values according to your needs. For more information, see Amazon RDS DB instance storage \u2014 Amazon Relational Database Service . Database name \u00b6 confluence_db_name defines the name of database to be used for the Confluence in RDS instance. If you restore the database, you need to provide the db name from the snapshot. If the snapshot does not have default db, then name set this variable to null . confluence_db_name = \"confluence\" Shared home configuration \u00b6 Shared home size \u00b6 confluence_shared_home_size sets the size of shared home storage in Gi. Default is 10Gi. confluence_shared_home_size = \"10Gi\" NFS server resource configuration \u00b6 NFS is used as shared home storage for Confluence. The deployment will create an NFS server within the cluster. The following variables set the initial cpu/memory request sizes including their limits for the NFS instance. (Default values used as example.) # Confluence NFS instance resource configuration confluence_nfs_requests_cpu = \"1\" confluence_nfs_requests_memory = \"1Gi\" confluence_nfs_limits_cpu = \"2\" confluence_nfs_limits_memory = \"2Gi\" S3 for attachment storage \u00b6 Since Confluence 8.1 AWS S3 can be used for storing and managing attachment data. confluence_s3_attachments_storage enables AWS S3 attachment storage, where Terraform will automatically create an S3 bucket, IAM role and policy. By default, this property is commented out. confluence_s3_attachments_storage = true Shared home is still required. S3 object storage is for attachment data only. You'll still need to use file system storage ( shared home ) for Confluence configuration data. Dataset restore configuration \u00b6 To restore the dataset into the newly created instance, configure all the parameters in this section. Database Snapshot Identifier \u00b6 confluence_db_snapshot_id sets the identifier of the DB snapshot to restore from. If you do not specify a value, no AWS RDS snapshot will be used. confluence_db_snapshot_id = \"<SNAPSHOT_IDENTIFIER>\" # e.g. \"my-snapshot\" The AWS RDS snapshot must be in the same region and account as the RDS instance to be created. You also need to provide the master user credentials ( confluence_db_master_username and confluence_db_master_password ) that match the snapshot. Optimise the restore performance. To obtain the best performance, configure Jira RDS that match the snapshot including confluence_db_instance_class and confluence_db_allocated_storage . Database Master Username \u00b6 confluence_db_master_username sets the username for the RDS master user. If you do not specify a value, username is \"postgres\". confluence_db_master_username = \"<DB_MASTER_USERNAME>\" # e.g. \"postgres\" Database Master Password \u00b6 confluence_db_master_password sets the password for the RDS master user. If you do not specify a value, a random password will be generated. confluence_db_master_password = \"<DB_MASTER_PASSWORD>\" # default value is null Build Number \u00b6 confluence_db_snapshot_build_number configures Confluence instance with the correct build number that stores in the snapshot. Without a matching build number, Confluence will not be able to start. List of build numbers . confluence_db_snapshot_build_number = \"<BUILD_NUMBER>\" # e.g. \"8703\" Shared home snapshot id \u00b6 To restore a shared home dataset, you can provide an EBS snapshot id that contains content of the shared home volume. This volume will then be mounted to the NFS server and used when the product is started. confluence_shared_home_snapshot_id sets the id of shared home EBS snapshot. Make sure the snapshot is available in the region you are deploying to and follows all product requirements. confluence_shared_home_snapshot_id = \"<SHARED_HOME_EBS_SNAPSHOT_IDENTIFIER>\" Snapshot and your environment must be in same region Search engine configuration \u00b6 Starting from Confluence 8.9.0, OpenSearch is supported as search engine. See: https://confluence.atlassian.com/doc/configuring-opensearch-for-confluence-1387594125.html OpenSearch \u00b6 confluence_opensearch_enabled decides whether to use OpenSearch as Confluence search engine. If set to true, a single-node OpenSearch will be created as part of the deployment, and Confluence will be configured to connect to this instance. confluence_opensearch_enabled = true OpenSearch instance resource configuration \u00b6 The following variables set number of CPU and amount of memory of OpenSearch instance. (Used default values as example.) confluence_opensearch_requests_cpu = \"1\" confluence_opensearch_requests_memory = \"1Gi\" OpenSearch persistent volume size \u00b6 confluence_opensearch_persistence_size sets the size of OpenSearch's persistence volume. (Used default values as example.) confluence_opensearch_persistence_size = \"10Gi\" OpenSearch initial admin password \u00b6 From OpenSearch Helm chart version 2.18.0 and App Version OpenSearch 2.12.0 onwards a custom strong password needs to be provided in order to setup demo admin user. If no password is specified, a random password will be generated. confluence_opensearch_initial_admin_password = \"OpenSearchAtl123!\" OpenSearch restore configuration \u00b6 To restore OpenSearch dataset, you can provide EBS snapshot ID of the OpenSearch volume. This volume will be used to pre-create OpenSearch PVC and PV. confluence_opensearch_snapshot_id sets the id of OpenSearch EBS snapshot. Make sure the snapshot is available in the region you are deploying to and follows all product requirements. confluence_opensearch_snapshot_id = \"<OPENSEARCH_SNAPSHOT_ID>\"","title":"Confluence configuration"},{"location":"userguide/configuration/CONFLUENCE_CONFIGURATION/#confluence-configuration","text":"","title":"Confluence configuration"},{"location":"userguide/configuration/CONFLUENCE_CONFIGURATION/#application-configuration","text":"","title":"Application configuration"},{"location":"userguide/configuration/CONFLUENCE_CONFIGURATION/#helm-chart-version","text":"confluence_helm_chart_version sets the Helm chart version of Confluence instance. confluence_helm_chart_version = \"1.2.0\"","title":"Helm chart version"},{"location":"userguide/configuration/CONFLUENCE_CONFIGURATION/#confluence-version-tag","text":"Confluence will be installed with the default version defined in its Helm chart . If you want to install a specific version of Bamboo, you can set the confluence_version_tag to the version you want to install. For more information, see Confluence Version Tags . confluence_version_tag = \"<CONFLUENCE_VERSION_TAG>\"","title":"Confluence version tag"},{"location":"userguide/configuration/CONFLUENCE_CONFIGURATION/#number-of-confluence-application-nodes","text":"The initial Confluence installation needs to be started with a single application node. After all the setup steps are finished, it is possible to update the confluence_replica_count with a number higher than 1 and run install.sh to update the application node count. # Number of Confluence application nodes # Note: For initial installation this value needs to be set to 1 and it can be changed only after Confluence is fully # installed and configured. confluence_replica_count = 1 Cluster size Cluster Autoscaler installed in the cluster will monitor the amount of required resources and adjust the cluster size to accomodate the requested cpu and memory.","title":"Number of Confluence application nodes"},{"location":"userguide/configuration/CONFLUENCE_CONFIGURATION/#installation-timeout","text":"confluence_installation_timeout defines the timeout (in minutes) for product helm chart installation . Different variables can influence how long it takes the application from installation to ready state. These can be dataset restoration, resource requirements, number of replicas and others. confluence_installation_timeout = 10","title":"Installation timeout"},{"location":"userguide/configuration/CONFLUENCE_CONFIGURATION/#license","text":"confluence_license takes the license key of Confluence product. Make sure that there is no new lines or spaces in license key. confluence_license = \"<LICENSE_KEY>\" Sensitive data confluence_license is marked as sensitive, storing in a plain-text config.tfvars file is not recommended. Please refer to Sensitive Data section.","title":"License"},{"location":"userguide/configuration/CONFLUENCE_CONFIGURATION/#confluence-instance-resource-configuration","text":"The following variables set number of CPU, amount of memory, maximum heap size and minimum heap size of Confluence instance. (Used default values as example.) confluence_cpu = \"2\" confluence_mem = \"1Gi\" confluence_min_heap = \"256m\" confluence_max_heap = \"512m\"","title":"Confluence instance resource configuration"},{"location":"userguide/configuration/CONFLUENCE_CONFIGURATION/#synchrony-instance-resource-configuration","text":"The following variables set number of CPU, amount of memory, maximum heap size, minimum heap size, and stack size of Synchrony instance. (Used default values as example.) synchrony_cpu = \"2\" synchrony_mem = \"2.5Gi\" synchrony_min_heap = \"1g\" synchrony_max_heap = \"2g\" synchrony_stack_size = \"2048k\"","title":"Synchrony instance resource configuration"},{"location":"userguide/configuration/CONFLUENCE_CONFIGURATION/#collaborative-editing","text":"confluence_collaborative_editing_enabled enables Collaborative editing . (default: true ) confluence_collaborative_editing_enabled = true","title":"Collaborative editing"},{"location":"userguide/configuration/CONFLUENCE_CONFIGURATION/#rds-configuration","text":"","title":"RDS Configuration"},{"location":"userguide/configuration/CONFLUENCE_CONFIGURATION/#database-engine-version","text":"confluence_db_major_engine_version sets the PostgeSQL engine version that will be used. confluence_db_major_engine_version = \"14\" Supported DB versions Be sure to use a DB engine version that is supported by Confluence Restore from snapshot This value is ignored if RDS snaphost is provided with confluence_db_snapshot_id .","title":"Database engine version"},{"location":"userguide/configuration/CONFLUENCE_CONFIGURATION/#database-instance-class","text":"confluence_db_instance_class sets the DB instance type that allocates the computational, network, and memory capacity required by the planned workload of the DB instance. For more information about available instance classes, see DB instance classes \u2014 Amazon Relational Database Service . confluence_db_instance_class = \"<INSTANCE_CLASS>\" # e.g. \"db.t3.micro\"","title":"Database Instance Class"},{"location":"userguide/configuration/CONFLUENCE_CONFIGURATION/#database-allocated-storage","text":"confluence_db_allocated_storage sets the allocated storage for the database instance in GiB. confluence_db_allocated_storage = 100 The allowed value range of allocated storage may vary based on instance class You may want to adjust these values according to your needs. For more information, see Amazon RDS DB instance storage \u2014 Amazon Relational Database Service .","title":"Database Allocated Storage"},{"location":"userguide/configuration/CONFLUENCE_CONFIGURATION/#database-iops","text":"confluence_db_iops sets the requested number of I/O operations per second that the DB instance can support. confluence_db_iops = 1000 The allowed value range of IOPS may vary based on instance class You may want to adjust these values according to your needs. For more information, see Amazon RDS DB instance storage \u2014 Amazon Relational Database Service .","title":"Database IOPS"},{"location":"userguide/configuration/CONFLUENCE_CONFIGURATION/#database-name","text":"confluence_db_name defines the name of database to be used for the Confluence in RDS instance. If you restore the database, you need to provide the db name from the snapshot. If the snapshot does not have default db, then name set this variable to null . confluence_db_name = \"confluence\"","title":"Database name"},{"location":"userguide/configuration/CONFLUENCE_CONFIGURATION/#shared-home-configuration","text":"","title":"Shared home configuration"},{"location":"userguide/configuration/CONFLUENCE_CONFIGURATION/#shared-home-size","text":"confluence_shared_home_size sets the size of shared home storage in Gi. Default is 10Gi. confluence_shared_home_size = \"10Gi\"","title":"Shared home size"},{"location":"userguide/configuration/CONFLUENCE_CONFIGURATION/#nfs-server-resource-configuration","text":"NFS is used as shared home storage for Confluence. The deployment will create an NFS server within the cluster. The following variables set the initial cpu/memory request sizes including their limits for the NFS instance. (Default values used as example.) # Confluence NFS instance resource configuration confluence_nfs_requests_cpu = \"1\" confluence_nfs_requests_memory = \"1Gi\" confluence_nfs_limits_cpu = \"2\" confluence_nfs_limits_memory = \"2Gi\"","title":"NFS server resource configuration"},{"location":"userguide/configuration/CONFLUENCE_CONFIGURATION/#s3-for-attachment-storage","text":"Since Confluence 8.1 AWS S3 can be used for storing and managing attachment data. confluence_s3_attachments_storage enables AWS S3 attachment storage, where Terraform will automatically create an S3 bucket, IAM role and policy. By default, this property is commented out. confluence_s3_attachments_storage = true Shared home is still required. S3 object storage is for attachment data only. You'll still need to use file system storage ( shared home ) for Confluence configuration data.","title":"S3 for attachment storage"},{"location":"userguide/configuration/CONFLUENCE_CONFIGURATION/#dataset-restore-configuration","text":"To restore the dataset into the newly created instance, configure all the parameters in this section.","title":"Dataset restore configuration"},{"location":"userguide/configuration/CONFLUENCE_CONFIGURATION/#database-snapshot-identifier","text":"confluence_db_snapshot_id sets the identifier of the DB snapshot to restore from. If you do not specify a value, no AWS RDS snapshot will be used. confluence_db_snapshot_id = \"<SNAPSHOT_IDENTIFIER>\" # e.g. \"my-snapshot\" The AWS RDS snapshot must be in the same region and account as the RDS instance to be created. You also need to provide the master user credentials ( confluence_db_master_username and confluence_db_master_password ) that match the snapshot. Optimise the restore performance. To obtain the best performance, configure Jira RDS that match the snapshot including confluence_db_instance_class and confluence_db_allocated_storage .","title":"Database Snapshot Identifier"},{"location":"userguide/configuration/CONFLUENCE_CONFIGURATION/#database-master-username","text":"confluence_db_master_username sets the username for the RDS master user. If you do not specify a value, username is \"postgres\". confluence_db_master_username = \"<DB_MASTER_USERNAME>\" # e.g. \"postgres\"","title":"Database Master Username"},{"location":"userguide/configuration/CONFLUENCE_CONFIGURATION/#database-master-password","text":"confluence_db_master_password sets the password for the RDS master user. If you do not specify a value, a random password will be generated. confluence_db_master_password = \"<DB_MASTER_PASSWORD>\" # default value is null","title":"Database Master Password"},{"location":"userguide/configuration/CONFLUENCE_CONFIGURATION/#build-number","text":"confluence_db_snapshot_build_number configures Confluence instance with the correct build number that stores in the snapshot. Without a matching build number, Confluence will not be able to start. List of build numbers . confluence_db_snapshot_build_number = \"<BUILD_NUMBER>\" # e.g. \"8703\"","title":"Build Number"},{"location":"userguide/configuration/CONFLUENCE_CONFIGURATION/#shared-home-snapshot-id","text":"To restore a shared home dataset, you can provide an EBS snapshot id that contains content of the shared home volume. This volume will then be mounted to the NFS server and used when the product is started. confluence_shared_home_snapshot_id sets the id of shared home EBS snapshot. Make sure the snapshot is available in the region you are deploying to and follows all product requirements. confluence_shared_home_snapshot_id = \"<SHARED_HOME_EBS_SNAPSHOT_IDENTIFIER>\" Snapshot and your environment must be in same region","title":"Shared home snapshot id"},{"location":"userguide/configuration/CONFLUENCE_CONFIGURATION/#search-engine-configuration","text":"Starting from Confluence 8.9.0, OpenSearch is supported as search engine. See: https://confluence.atlassian.com/doc/configuring-opensearch-for-confluence-1387594125.html","title":"Search engine configuration"},{"location":"userguide/configuration/CONFLUENCE_CONFIGURATION/#opensearch","text":"confluence_opensearch_enabled decides whether to use OpenSearch as Confluence search engine. If set to true, a single-node OpenSearch will be created as part of the deployment, and Confluence will be configured to connect to this instance. confluence_opensearch_enabled = true","title":"OpenSearch"},{"location":"userguide/configuration/CONFLUENCE_CONFIGURATION/#opensearch-instance-resource-configuration","text":"The following variables set number of CPU and amount of memory of OpenSearch instance. (Used default values as example.) confluence_opensearch_requests_cpu = \"1\" confluence_opensearch_requests_memory = \"1Gi\"","title":"OpenSearch instance resource configuration"},{"location":"userguide/configuration/CONFLUENCE_CONFIGURATION/#opensearch-persistent-volume-size","text":"confluence_opensearch_persistence_size sets the size of OpenSearch's persistence volume. (Used default values as example.) confluence_opensearch_persistence_size = \"10Gi\"","title":"OpenSearch persistent volume size"},{"location":"userguide/configuration/CONFLUENCE_CONFIGURATION/#opensearch-initial-admin-password","text":"From OpenSearch Helm chart version 2.18.0 and App Version OpenSearch 2.12.0 onwards a custom strong password needs to be provided in order to setup demo admin user. If no password is specified, a random password will be generated. confluence_opensearch_initial_admin_password = \"OpenSearchAtl123!\"","title":"OpenSearch initial admin password"},{"location":"userguide/configuration/CONFLUENCE_CONFIGURATION/#opensearch-restore-configuration","text":"To restore OpenSearch dataset, you can provide EBS snapshot ID of the OpenSearch volume. This volume will be used to pre-create OpenSearch PVC and PV. confluence_opensearch_snapshot_id sets the id of OpenSearch EBS snapshot. Make sure the snapshot is available in the region you are deploying to and follows all product requirements. confluence_opensearch_snapshot_id = \"<OPENSEARCH_SNAPSHOT_ID>\"","title":"OpenSearch restore configuration"},{"location":"userguide/configuration/CROWD_CONFIGURATION/","text":"Crowd configuration \u00b6 Application configuration \u00b6 Helm chart version \u00b6 crowd_helm_chart_version sets the Helm chart version of Crowd instance. crowd_helm_chart_version = \"1.10.0\" Crowd version tag \u00b6 Crowd will be installed with the default version defined in its Helm chart . If you want to install a specific version of Crowd, you can set the crowd_version_tag to the version you want to install. For more information, see Crowd Version Tags . crowd_version_tag = \"<Crowd_VERSION_TAG>\" Number of Crowd application nodes \u00b6 The initial Crowd installation needs to be started with a single application node. After all the setup steps are finished, it is possible to update the crowd_replica_count with a number higher than 1 and run install.sh to update the application node count. # Number of Crowd application nodes # Note: For initial installation this value needs to be set to 1 and it can be changed only after Crowd is fully # installed and configured. crowd_replica_count = 1 Cluster size Cluster Autoscaler installed in the cluster will monitor the amount of required resources and adjust the cluster size to accomodate the requested cpu and memory. Installation timeout \u00b6 crowd_installation_timeout defines the timeout (in minutes) for product helm chart installation . Different variables can influence how long it takes the application from installation to ready state. These can be dataset restoration, resource requirements, number of replicas and others. crowd_installation_timeout = 10 Crowd instance resource configuration \u00b6 The following variables set number of CPU, amount of memory, maximum heap size and minimum heap size of Crowd instance. (Used default values as example.) crowd_cpu = \"2\" crowd_mem = \"1Gi\" crowd_min_heap = \"256m\" crowd_max_heap = \"512m\" RDS Configuration \u00b6 Database engine version \u00b6 crowd_db_major_engine_version sets the PostgeSQL engine version that will be used. crowd_db_major_engine_version = \"13\" Supported DB versions Be sure to use a DB engine version that is supported by crowd Restore from snapshot This value is ignored if RDS snaphost is provided with crowd_db_snapshot_id . Database Instance Class \u00b6 crowd_db_instance_class sets the DB instance type that allocates the computational, network, and memory capacity required by the planned workload of the DB instance. For more information about available instance classes, see DB instance classes \u2014 Amazon Relational Database Service . crowd_db_instance_class = \"<INSTANCE_CLASS>\" # e.g. \"db.t3.micro\" Database Allocated Storage \u00b6 crowd_db_allocated_storage sets the allocated storage for the database instance in GiB. crowd_db_allocated_storage = 100 The allowed value range of allocated storage may vary based on instance class You may want to adjust these values according to your needs. For more information, see Amazon RDS DB instance storage \u2014 Amazon Relational Database Service . Database IOPS \u00b6 crowd_db_iops sets the requested number of I/O operations per second that the DB instance can support. crowd_db_iops = 1000 The allowed value range of IOPS may vary based on instance class You may want to adjust these values according to your needs. For more information, see Amazon RDS DB instance storage \u2014 Amazon Relational Database Service . Database name \u00b6 crowd_db_name defines the name of database to be used for the crowd in RDS instance. If you restore the database, you need to provide the db name from the snapshot. If the snapshot does not have default db, then name set this variable to null . crowd_db_name = \"crowd\" Shared home configuration \u00b6 Shared home size \u00b6 crowd_shared_home_size sets the size of shared home storage in Gi. Default is 10Gi. crowd_shared_home_size = \"10Gi\" Local home configuration \u00b6 Local home size \u00b6 crowd_local_home_size sets the size of shared home storage in Gi. Default is 10Gi. crowd_local_home_size = \"10Gi\" NFS server resource configuration \u00b6 NFS is used as shared home storage for Crowd. The deployment will create an NFS server within the cluster. The following variables set the initial cpu/memory request sizes including their limits for the NFS instance. (Default values used as example.) # crowd NFS instance resource configuration crowd_nfs_requests_cpu = \"1\" crowd_nfs_requests_memory = \"1Gi\" crowd_nfs_limits_cpu = \"2\" crowd_nfs_limits_memory = \"2Gi\" Dataset restore configuration \u00b6 To restore the dataset into the newly created instance, configure all the parameters in this section. Database Snapshot Identifier \u00b6 crowd_db_snapshot_id sets the identifier of the DB snapshot to restore from. If you do not specify a value, no AWS RDS snapshot will be used. crowd_db_snapshot_id = \"<SNAPSHOT_IDENTIFIER>\" # e.g. \"my-snapshot\" The AWS RDS snapshot must be in the same region and account as the RDS instance to be created. You also need to provide the master user credentials ( crowd_db_master_username and crowd_db_master_password ) that match the snapshot. Optimise the restore performance. To obtain the best performance, configure Crowd RDS that match the snapshot including crowd_db_instance_class and crowd_db_allocated_storage . Database Master Username \u00b6 crowd_db_master_username sets the username for the RDS master user. If you do not specify a value, username is \"postgres\". crowd_db_master_username = \"<DB_MASTER_USERNAME>\" # e.g. \"postgres\" Database Master Password \u00b6 crowd_db_master_password sets the password for the RDS master user. If you do not specify a value, a random password will be generated. crowd_db_master_password = \"<DB_MASTER_PASSWORD>\" # default value is null Build Number \u00b6 crowd_db_snapshot_build_number configures Crowd instance with the correct build number that stores in the snapshot. Without a matching build number, Crowd will not be able to start. List of build numbers . crowd_db_snapshot_build_number = \"<BUILD_NUMBER>\" # e.g. \"8703\" Shared home snapshot id \u00b6 To restore a shared home dataset, you can provide an EBS snapshot id that contains content of the shared home volume. This volume will then be mounted to the NFS server and used when the product is started. crowd_shared_home_snapshot_id sets the id of shared home EBS snapshot. Make sure the snapshot is available in the region you are deploying to and follows all product requirements. crowd_shared_home_snapshot_id = \"<SHARED_HOME_EBS_SNAPSHOT_IDENTIFIER>\" Snapshot and your environment must be in same region","title":"Crowd configuration"},{"location":"userguide/configuration/CROWD_CONFIGURATION/#crowd-configuration","text":"","title":"Crowd configuration"},{"location":"userguide/configuration/CROWD_CONFIGURATION/#application-configuration","text":"","title":"Application configuration"},{"location":"userguide/configuration/CROWD_CONFIGURATION/#helm-chart-version","text":"crowd_helm_chart_version sets the Helm chart version of Crowd instance. crowd_helm_chart_version = \"1.10.0\"","title":"Helm chart version"},{"location":"userguide/configuration/CROWD_CONFIGURATION/#crowd-version-tag","text":"Crowd will be installed with the default version defined in its Helm chart . If you want to install a specific version of Crowd, you can set the crowd_version_tag to the version you want to install. For more information, see Crowd Version Tags . crowd_version_tag = \"<Crowd_VERSION_TAG>\"","title":"Crowd version tag"},{"location":"userguide/configuration/CROWD_CONFIGURATION/#number-of-crowd-application-nodes","text":"The initial Crowd installation needs to be started with a single application node. After all the setup steps are finished, it is possible to update the crowd_replica_count with a number higher than 1 and run install.sh to update the application node count. # Number of Crowd application nodes # Note: For initial installation this value needs to be set to 1 and it can be changed only after Crowd is fully # installed and configured. crowd_replica_count = 1 Cluster size Cluster Autoscaler installed in the cluster will monitor the amount of required resources and adjust the cluster size to accomodate the requested cpu and memory.","title":"Number of Crowd application nodes"},{"location":"userguide/configuration/CROWD_CONFIGURATION/#installation-timeout","text":"crowd_installation_timeout defines the timeout (in minutes) for product helm chart installation . Different variables can influence how long it takes the application from installation to ready state. These can be dataset restoration, resource requirements, number of replicas and others. crowd_installation_timeout = 10","title":"Installation timeout"},{"location":"userguide/configuration/CROWD_CONFIGURATION/#crowd-instance-resource-configuration","text":"The following variables set number of CPU, amount of memory, maximum heap size and minimum heap size of Crowd instance. (Used default values as example.) crowd_cpu = \"2\" crowd_mem = \"1Gi\" crowd_min_heap = \"256m\" crowd_max_heap = \"512m\"","title":"Crowd instance resource configuration"},{"location":"userguide/configuration/CROWD_CONFIGURATION/#rds-configuration","text":"","title":"RDS Configuration"},{"location":"userguide/configuration/CROWD_CONFIGURATION/#database-engine-version","text":"crowd_db_major_engine_version sets the PostgeSQL engine version that will be used. crowd_db_major_engine_version = \"13\" Supported DB versions Be sure to use a DB engine version that is supported by crowd Restore from snapshot This value is ignored if RDS snaphost is provided with crowd_db_snapshot_id .","title":"Database engine version"},{"location":"userguide/configuration/CROWD_CONFIGURATION/#database-instance-class","text":"crowd_db_instance_class sets the DB instance type that allocates the computational, network, and memory capacity required by the planned workload of the DB instance. For more information about available instance classes, see DB instance classes \u2014 Amazon Relational Database Service . crowd_db_instance_class = \"<INSTANCE_CLASS>\" # e.g. \"db.t3.micro\"","title":"Database Instance Class"},{"location":"userguide/configuration/CROWD_CONFIGURATION/#database-allocated-storage","text":"crowd_db_allocated_storage sets the allocated storage for the database instance in GiB. crowd_db_allocated_storage = 100 The allowed value range of allocated storage may vary based on instance class You may want to adjust these values according to your needs. For more information, see Amazon RDS DB instance storage \u2014 Amazon Relational Database Service .","title":"Database Allocated Storage"},{"location":"userguide/configuration/CROWD_CONFIGURATION/#database-iops","text":"crowd_db_iops sets the requested number of I/O operations per second that the DB instance can support. crowd_db_iops = 1000 The allowed value range of IOPS may vary based on instance class You may want to adjust these values according to your needs. For more information, see Amazon RDS DB instance storage \u2014 Amazon Relational Database Service .","title":"Database IOPS"},{"location":"userguide/configuration/CROWD_CONFIGURATION/#database-name","text":"crowd_db_name defines the name of database to be used for the crowd in RDS instance. If you restore the database, you need to provide the db name from the snapshot. If the snapshot does not have default db, then name set this variable to null . crowd_db_name = \"crowd\"","title":"Database name"},{"location":"userguide/configuration/CROWD_CONFIGURATION/#shared-home-configuration","text":"","title":"Shared home configuration"},{"location":"userguide/configuration/CROWD_CONFIGURATION/#shared-home-size","text":"crowd_shared_home_size sets the size of shared home storage in Gi. Default is 10Gi. crowd_shared_home_size = \"10Gi\"","title":"Shared home size"},{"location":"userguide/configuration/CROWD_CONFIGURATION/#local-home-configuration","text":"","title":"Local home configuration"},{"location":"userguide/configuration/CROWD_CONFIGURATION/#local-home-size","text":"crowd_local_home_size sets the size of shared home storage in Gi. Default is 10Gi. crowd_local_home_size = \"10Gi\"","title":"Local home size"},{"location":"userguide/configuration/CROWD_CONFIGURATION/#nfs-server-resource-configuration","text":"NFS is used as shared home storage for Crowd. The deployment will create an NFS server within the cluster. The following variables set the initial cpu/memory request sizes including their limits for the NFS instance. (Default values used as example.) # crowd NFS instance resource configuration crowd_nfs_requests_cpu = \"1\" crowd_nfs_requests_memory = \"1Gi\" crowd_nfs_limits_cpu = \"2\" crowd_nfs_limits_memory = \"2Gi\"","title":"NFS server resource configuration"},{"location":"userguide/configuration/CROWD_CONFIGURATION/#dataset-restore-configuration","text":"To restore the dataset into the newly created instance, configure all the parameters in this section.","title":"Dataset restore configuration"},{"location":"userguide/configuration/CROWD_CONFIGURATION/#database-snapshot-identifier","text":"crowd_db_snapshot_id sets the identifier of the DB snapshot to restore from. If you do not specify a value, no AWS RDS snapshot will be used. crowd_db_snapshot_id = \"<SNAPSHOT_IDENTIFIER>\" # e.g. \"my-snapshot\" The AWS RDS snapshot must be in the same region and account as the RDS instance to be created. You also need to provide the master user credentials ( crowd_db_master_username and crowd_db_master_password ) that match the snapshot. Optimise the restore performance. To obtain the best performance, configure Crowd RDS that match the snapshot including crowd_db_instance_class and crowd_db_allocated_storage .","title":"Database Snapshot Identifier"},{"location":"userguide/configuration/CROWD_CONFIGURATION/#database-master-username","text":"crowd_db_master_username sets the username for the RDS master user. If you do not specify a value, username is \"postgres\". crowd_db_master_username = \"<DB_MASTER_USERNAME>\" # e.g. \"postgres\"","title":"Database Master Username"},{"location":"userguide/configuration/CROWD_CONFIGURATION/#database-master-password","text":"crowd_db_master_password sets the password for the RDS master user. If you do not specify a value, a random password will be generated. crowd_db_master_password = \"<DB_MASTER_PASSWORD>\" # default value is null","title":"Database Master Password"},{"location":"userguide/configuration/CROWD_CONFIGURATION/#build-number","text":"crowd_db_snapshot_build_number configures Crowd instance with the correct build number that stores in the snapshot. Without a matching build number, Crowd will not be able to start. List of build numbers . crowd_db_snapshot_build_number = \"<BUILD_NUMBER>\" # e.g. \"8703\"","title":"Build Number"},{"location":"userguide/configuration/CROWD_CONFIGURATION/#shared-home-snapshot-id","text":"To restore a shared home dataset, you can provide an EBS snapshot id that contains content of the shared home volume. This volume will then be mounted to the NFS server and used when the product is started. crowd_shared_home_snapshot_id sets the id of shared home EBS snapshot. Make sure the snapshot is available in the region you are deploying to and follows all product requirements. crowd_shared_home_snapshot_id = \"<SHARED_HOME_EBS_SNAPSHOT_IDENTIFIER>\" Snapshot and your environment must be in same region","title":"Shared home snapshot id"},{"location":"userguide/configuration/DCAPT_CONFIGURATION/","text":"DCAPT Configuration \u00b6 Besides deploying Atlassian DC products, it is possible to spin up a testing environment to run Data Center App Performance Toolkit tests . Create DCAPT Test Deployment \u00b6 Create an additional deployment for DCAPT Jmeter and Selenium test environment: start_test_deployment = true Configure DCAPT Test Deployment \u00b6 The default values works well with a typical DCAPT test, however, if you need to allocate more resources to the test container, use the following variables to override the defaults: Configure CPU \u00b6 test_deployment_cpu_request = \"1\" test_deployment_cpu_limit = \"4\" Configure Memory \u00b6 test_deployment_mem_request = \"4Gi\" test_deployment_mem_limit = \"6Gi\" Configure Image and Tag \u00b6 The container starts in a privileged mode to be able to run docker-in-docker. If you need to change the image repository or tag, use the following variables: test_deployment_image_repo = \"docker\" test_deployment_image_tag = \"24.0.7-dind\"","title":"DCAPT Configuration"},{"location":"userguide/configuration/DCAPT_CONFIGURATION/#dcapt-configuration","text":"Besides deploying Atlassian DC products, it is possible to spin up a testing environment to run Data Center App Performance Toolkit tests .","title":"DCAPT Configuration"},{"location":"userguide/configuration/DCAPT_CONFIGURATION/#create-dcapt-test-deployment","text":"Create an additional deployment for DCAPT Jmeter and Selenium test environment: start_test_deployment = true","title":"Create DCAPT Test Deployment"},{"location":"userguide/configuration/DCAPT_CONFIGURATION/#configure-dcapt-test-deployment","text":"The default values works well with a typical DCAPT test, however, if you need to allocate more resources to the test container, use the following variables to override the defaults:","title":"Configure DCAPT Test Deployment"},{"location":"userguide/configuration/DCAPT_CONFIGURATION/#configure-cpu","text":"test_deployment_cpu_request = \"1\" test_deployment_cpu_limit = \"4\"","title":"Configure CPU"},{"location":"userguide/configuration/DCAPT_CONFIGURATION/#configure-memory","text":"test_deployment_mem_request = \"4Gi\" test_deployment_mem_limit = \"6Gi\"","title":"Configure Memory"},{"location":"userguide/configuration/DCAPT_CONFIGURATION/#configure-image-and-tag","text":"The container starts in a privileged mode to be able to run docker-in-docker. If you need to change the image repository or tag, use the following variables: test_deployment_image_repo = \"docker\" test_deployment_image_tag = \"24.0.7-dind\"","title":"Configure Image and Tag"},{"location":"userguide/configuration/JIRA_CONFIGURATION/","text":"Jira configuration \u00b6 Jira Service Management Jira Service Management DC deployment is configured the same as a Jira deployment with several specific configuration values. products = [\"jira\"] jira_image_repository = \"atlassian/jira-servicemanagement\" jira_version_tag = \"4.20\" # or other compatible version Application configuration \u00b6 Helm chart version \u00b6 jira_helm_chart_version sets the Helm chart version of Jira instance. jira_helm_chart_version = \"1.2.0\" Jira version tag \u00b6 Jira Software will be installed with the default version defined in its Helm chart . If you want to install a specific version of Jira software, you can set the jira_version_tag to the version you want to install. For more information, see Jira Version Tags . jira_version_tag = \"<JIRA_VERSION_TAG>\" Jira image repository \u00b6 To change the Jira edition you can set a different image repository. By default, Jira Software edition is installed. You need to make sure the appropriate version defined in jira_version_tag is available for the selected Jira edition. See below the available tags for each edition. jira_image_repository = \"<JIRA_IMAGE_REPOSITORY>\" Supported image repository values atlassian/jira-software atlassian/jira-servicemanagement Number of Jira application nodes \u00b6 The initial Jira installation needs to be started with a single application node. After all the setup steps are finished, it is possible to update the jira_replica_count with a number higher than 1 and run install.sh to update the application node count. # Number of Jira application nodes # Note: For initial installation this value needs to be set to 1 and it can be changed only after Jira is fully # installed and configured. jira_replica_count = 1 Cluster size Cluster Autoscaler installed in the cluster will monitor the amount of required resources and adjust the cluster size to accomodate the requested cpu and memory. Installation timeout \u00b6 jira_installation_timeout defines the timeout (in minutes) for product helm chart installation . Different variables can influence how long it takes the application from installation to ready state. These can be dataset restoration, resource requirements, number of replicas and others. jira_installation_timeout = 10 Instance resource configuration \u00b6 The following variables set number of CPU, amount of memory, maximum heap size and minimum heap size of Jira instance. (Used default values as example.) jira_cpu = \"1\" jira_mem = \"2Gi\" jira_min_heap = \"384m\" jira_max_heap = \"786m\" jira_reserved_code_cache = \"512m\" RDS configuration \u00b6 Database engine version \u00b6 jira_db_major_engine_version sets the PostgeSQL engine version that will be used. jira_db_major_engine_version = \"14\" Supported DB versions Be sure to use a DB engine version that is supported by Jira Restore from snapshot This value is ignored if RDS snaphost is provided with jira_db_snapshot_id . Database Instance Class \u00b6 jira_db_instance_class sets the DB instance type that allocates the computational, network, and memory capacity required by the planned workload of the DB instance. For more information about available instance classes, see DB instance classes \u2014 Amazon Relational Database Service . jira_db_instance_class = \"<INSTANCE_CLASS>\" # e.g. \"db.t3.micro\" Database Allocated Storage \u00b6 jira_db_allocated_storage sets the allocated storage for the database instance in GiB. jira_db_allocated_storage = 100 The allowed value range of allocated storage may vary based on instance class You may want to adjust these values according to your needs. For more information, see Amazon RDS DB instance storage \u2014 Amazon Relational Database Service . Database IOPS \u00b6 jira_db_iops sets the requested number of I/O operations per second that the DB instance can support. jira_db_iops = 1000 The allowed value range of IOPS may vary based on instance class You may want to adjust these values according to your needs. For more information, see Amazon RDS DB instance storage \u2014 Amazon Relational Database Service . Database name \u00b6 jira_db_name defines the name of database to be used for the Jira in RDS instance. If you restore the database, you need to provide the db name from the snapshot. If the snapshot does not have default db name, then set this variable to null . jira_db_name = \"jira\" NFS configuration \u00b6 NFS resource configuration \u00b6 The following variables set the initial cpu/memory request sizes including their limits for the NFS instance. (Default values used as example.) # Jira NFS instance resource configuration jira_nfs_requests_cpu = \"1\" jira_nfs_requests_memory = \"1Gi\" jira_nfs_limits_cpu = \"2\" jira_nfs_limits_memory = \"2Gi\" Dataset restore configuration \u00b6 To restore the dataset into the newly created instance, uncomment the following lines and provide all necessary parameters. Database Snapshot Identifier and Jira license \u00b6 jira_db_snapshot_id sets the identifier for the DB snapshot to restore from. If you do not specify a value, no AWS RDS snapshot is used. jira_db_snapshot_id = \"<SNAPSHOT_IDENTIFIER>\" # e.g. \"my-snapshot\" jira_license takes the license key of Jira product. you must provide Jira license key when a RDS snapshot is used. jira_license = \"<LICENSE_KEY>\" The AWS RDS snapshot must be in the same region and account as the RDS instance. You also need to provide the master user credentials ( jira_db_master_username and jira_db_master_password ) that match the snapshot. Optimise the restore performance. To obtain the best performance, configure Jira RDS that match the snapshot including jira_db_instance_class and jira_db_allocated_storage . Jira license limitation you can provide jira_license ONLY when a RDS snapshot is used. If you plans to provision a new RDS instance comment out jira_license and add the license key manually via application UI. Please refer to Sensitive Data section. Database Master Username \u00b6 jira_db_master_username sets the username for the RDS master user. If you do not specify a value, username is \"postgres\". jira_db_master_username = \"<DB_MASTER_USERNAME>\" # e.g. \"postgres\" Database Master Password \u00b6 jira_db_master_password sets the password for the RDS master user. If you do not specify a value, a random password will be generated. jira_db_master_password = \"<DB_MASTER_PASSWORD>\" # default value is null Shared Home Restore \u00b6 jira_shared_home_snapshot_id sets the id of the shared home EBS snapshot to use. This will spin up an EBS volume that will be mounted to the NFS server and used when the product is started. jira_shared_home_snapshot_id = \"<SHARED_HOME_EBS_SNAPSHOT_IDENTIFIER>\" Snapshot and your environment must be in same region","title":"Jira configuration"},{"location":"userguide/configuration/JIRA_CONFIGURATION/#jira-configuration","text":"Jira Service Management Jira Service Management DC deployment is configured the same as a Jira deployment with several specific configuration values. products = [\"jira\"] jira_image_repository = \"atlassian/jira-servicemanagement\" jira_version_tag = \"4.20\" # or other compatible version","title":"Jira configuration"},{"location":"userguide/configuration/JIRA_CONFIGURATION/#application-configuration","text":"","title":"Application configuration"},{"location":"userguide/configuration/JIRA_CONFIGURATION/#helm-chart-version","text":"jira_helm_chart_version sets the Helm chart version of Jira instance. jira_helm_chart_version = \"1.2.0\"","title":"Helm chart version"},{"location":"userguide/configuration/JIRA_CONFIGURATION/#jira-version-tag","text":"Jira Software will be installed with the default version defined in its Helm chart . If you want to install a specific version of Jira software, you can set the jira_version_tag to the version you want to install. For more information, see Jira Version Tags . jira_version_tag = \"<JIRA_VERSION_TAG>\"","title":"Jira version tag"},{"location":"userguide/configuration/JIRA_CONFIGURATION/#jira-image-repository","text":"To change the Jira edition you can set a different image repository. By default, Jira Software edition is installed. You need to make sure the appropriate version defined in jira_version_tag is available for the selected Jira edition. See below the available tags for each edition. jira_image_repository = \"<JIRA_IMAGE_REPOSITORY>\" Supported image repository values atlassian/jira-software atlassian/jira-servicemanagement","title":"Jira image repository"},{"location":"userguide/configuration/JIRA_CONFIGURATION/#number-of-jira-application-nodes","text":"The initial Jira installation needs to be started with a single application node. After all the setup steps are finished, it is possible to update the jira_replica_count with a number higher than 1 and run install.sh to update the application node count. # Number of Jira application nodes # Note: For initial installation this value needs to be set to 1 and it can be changed only after Jira is fully # installed and configured. jira_replica_count = 1 Cluster size Cluster Autoscaler installed in the cluster will monitor the amount of required resources and adjust the cluster size to accomodate the requested cpu and memory.","title":"Number of Jira application nodes"},{"location":"userguide/configuration/JIRA_CONFIGURATION/#installation-timeout","text":"jira_installation_timeout defines the timeout (in minutes) for product helm chart installation . Different variables can influence how long it takes the application from installation to ready state. These can be dataset restoration, resource requirements, number of replicas and others. jira_installation_timeout = 10","title":"Installation timeout"},{"location":"userguide/configuration/JIRA_CONFIGURATION/#instance-resource-configuration","text":"The following variables set number of CPU, amount of memory, maximum heap size and minimum heap size of Jira instance. (Used default values as example.) jira_cpu = \"1\" jira_mem = \"2Gi\" jira_min_heap = \"384m\" jira_max_heap = \"786m\" jira_reserved_code_cache = \"512m\"","title":"Instance resource configuration"},{"location":"userguide/configuration/JIRA_CONFIGURATION/#rds-configuration","text":"","title":"RDS configuration"},{"location":"userguide/configuration/JIRA_CONFIGURATION/#database-engine-version","text":"jira_db_major_engine_version sets the PostgeSQL engine version that will be used. jira_db_major_engine_version = \"14\" Supported DB versions Be sure to use a DB engine version that is supported by Jira Restore from snapshot This value is ignored if RDS snaphost is provided with jira_db_snapshot_id .","title":"Database engine version"},{"location":"userguide/configuration/JIRA_CONFIGURATION/#database-instance-class","text":"jira_db_instance_class sets the DB instance type that allocates the computational, network, and memory capacity required by the planned workload of the DB instance. For more information about available instance classes, see DB instance classes \u2014 Amazon Relational Database Service . jira_db_instance_class = \"<INSTANCE_CLASS>\" # e.g. \"db.t3.micro\"","title":"Database Instance Class"},{"location":"userguide/configuration/JIRA_CONFIGURATION/#database-allocated-storage","text":"jira_db_allocated_storage sets the allocated storage for the database instance in GiB. jira_db_allocated_storage = 100 The allowed value range of allocated storage may vary based on instance class You may want to adjust these values according to your needs. For more information, see Amazon RDS DB instance storage \u2014 Amazon Relational Database Service .","title":"Database Allocated Storage"},{"location":"userguide/configuration/JIRA_CONFIGURATION/#database-iops","text":"jira_db_iops sets the requested number of I/O operations per second that the DB instance can support. jira_db_iops = 1000 The allowed value range of IOPS may vary based on instance class You may want to adjust these values according to your needs. For more information, see Amazon RDS DB instance storage \u2014 Amazon Relational Database Service .","title":"Database IOPS"},{"location":"userguide/configuration/JIRA_CONFIGURATION/#database-name","text":"jira_db_name defines the name of database to be used for the Jira in RDS instance. If you restore the database, you need to provide the db name from the snapshot. If the snapshot does not have default db name, then set this variable to null . jira_db_name = \"jira\"","title":"Database name"},{"location":"userguide/configuration/JIRA_CONFIGURATION/#nfs-configuration","text":"","title":"NFS configuration"},{"location":"userguide/configuration/JIRA_CONFIGURATION/#nfs-resource-configuration","text":"The following variables set the initial cpu/memory request sizes including their limits for the NFS instance. (Default values used as example.) # Jira NFS instance resource configuration jira_nfs_requests_cpu = \"1\" jira_nfs_requests_memory = \"1Gi\" jira_nfs_limits_cpu = \"2\" jira_nfs_limits_memory = \"2Gi\"","title":"NFS resource configuration"},{"location":"userguide/configuration/JIRA_CONFIGURATION/#dataset-restore-configuration","text":"To restore the dataset into the newly created instance, uncomment the following lines and provide all necessary parameters.","title":"Dataset restore configuration"},{"location":"userguide/configuration/JIRA_CONFIGURATION/#database-snapshot-identifier-and-jira-license","text":"jira_db_snapshot_id sets the identifier for the DB snapshot to restore from. If you do not specify a value, no AWS RDS snapshot is used. jira_db_snapshot_id = \"<SNAPSHOT_IDENTIFIER>\" # e.g. \"my-snapshot\" jira_license takes the license key of Jira product. you must provide Jira license key when a RDS snapshot is used. jira_license = \"<LICENSE_KEY>\" The AWS RDS snapshot must be in the same region and account as the RDS instance. You also need to provide the master user credentials ( jira_db_master_username and jira_db_master_password ) that match the snapshot. Optimise the restore performance. To obtain the best performance, configure Jira RDS that match the snapshot including jira_db_instance_class and jira_db_allocated_storage . Jira license limitation you can provide jira_license ONLY when a RDS snapshot is used. If you plans to provision a new RDS instance comment out jira_license and add the license key manually via application UI. Please refer to Sensitive Data section.","title":"Database Snapshot Identifier and Jira license"},{"location":"userguide/configuration/JIRA_CONFIGURATION/#database-master-username","text":"jira_db_master_username sets the username for the RDS master user. If you do not specify a value, username is \"postgres\". jira_db_master_username = \"<DB_MASTER_USERNAME>\" # e.g. \"postgres\"","title":"Database Master Username"},{"location":"userguide/configuration/JIRA_CONFIGURATION/#database-master-password","text":"jira_db_master_password sets the password for the RDS master user. If you do not specify a value, a random password will be generated. jira_db_master_password = \"<DB_MASTER_PASSWORD>\" # default value is null","title":"Database Master Password"},{"location":"userguide/configuration/JIRA_CONFIGURATION/#shared-home-restore","text":"jira_shared_home_snapshot_id sets the id of the shared home EBS snapshot to use. This will spin up an EBS volume that will be mounted to the NFS server and used when the product is started. jira_shared_home_snapshot_id = \"<SHARED_HOME_EBS_SNAPSHOT_IDENTIFIER>\" Snapshot and your environment must be in same region","title":"Shared Home Restore"}]}